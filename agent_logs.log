2024-10-30 17:45:25,235 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-30 17:45:31,822 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': 'hi', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:45:31,824 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:45:31,968 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a4ef7c10>
2024-10-30 17:45:31,968 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd1c00a3040> server_hostname='api.openai.com' timeout=None
2024-10-30 17:45:32,019 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a4ef76a0>
2024-10-30 17:45:32,019 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:45:32,020 - DEBUG - send_request_headers.complete
2024-10-30 17:45:32,020 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:45:32,021 - DEBUG - send_request_body.complete
2024-10-30 17:45:32,021 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:45:32,885 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:45:32 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'594'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'449957'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'5ms'), (b'x-request-id', b'req_e36d136feec225b3f89077b5fecb95f4'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=5X8a_FHswHA7wf3tKH2fk3INdf0tu21uR5qssHiyLyA-1730324732-1.0.1.1-ODXe3qTpNzSabFq2wQTl6JT.8.yaauFJJarquOEmfrtMPYdv9bbwajEQXmGXF1.ySOnTcbNJ_FfNVX5QI2i33Q; path=/; expires=Wed, 30-Oct-24 22:15:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=WRy7oShLvPXFlPFUy2.9mnI2Syh5E6d5AxV7WHqX16c-1730324732876-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8dae97c7de278dd0-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:45:32,886 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:45:32,887 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:45:32,887 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:45:33,098 - DEBUG - receive_response_body.complete
2024-10-30 17:45:33,099 - DEBUG - response_closed.started
2024-10-30 17:45:33,104 - DEBUG - response_closed.complete
2024-10-30 17:45:33,121 - DEBUG - Raw Message from LangChain: content='Hello! How can I assist you today?' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0125-preview'} id='run-8f1e9a14-e1ee-4cbc-a927-d8fe04eca1b1-0'
2024-10-30 17:45:33,123 - INFO - Structured Response: {'node': 'agent', 'message': {'content': 'Hello! How can I assist you today?', 'type': 'AIMessage', 'additional_kwargs': {'type': 'AIMessage'}}}
2024-10-30 17:45:42,376 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': 'hi', 'role': 'user'}, {'content': 'Hello! How can I assist you today?', 'role': 'assistant'}, {'content': 'what different types of streaming are in langgraph?', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:45:42,378 - DEBUG - close.started
2024-10-30 17:45:42,379 - DEBUG - close.complete
2024-10-30 17:45:42,379 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:45:42,491 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a4eb1150>
2024-10-30 17:45:42,491 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd1c00a3040> server_hostname='api.openai.com' timeout=None
2024-10-30 17:45:42,548 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a4ef4910>
2024-10-30 17:45:42,548 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:45:42,549 - DEBUG - send_request_headers.complete
2024-10-30 17:45:42,549 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:45:42,549 - DEBUG - send_request_body.complete
2024-10-30 17:45:42,549 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:45:43,591 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:45:43 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'771'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'449933'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'8ms'), (b'x-request-id', b'req_61bde2f552f802b199552def34ed916e'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8dae980999323358-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:45:43,592 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:45:43,592 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:45:43,593 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:45:43,858 - DEBUG - receive_response_body.complete
2024-10-30 17:45:43,858 - DEBUG - response_closed.started
2024-10-30 17:45:43,859 - DEBUG - response_closed.complete
2024-10-30 17:45:43,863 - DEBUG - Raw Message from LangChain: content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_fhfas8Oy7jE8i9brDiPEGtmh', 'function': {'arguments': '{"query":"types of streaming in LangGraph"}', 'name': 'retrieve_docs'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-0125-preview'} id='run-2b05b0c7-2ce6-4fb5-aa8c-65fa2de2020d-0' tool_calls=[{'name': 'retrieve_docs', 'args': {'query': 'types of streaming in LangGraph'}, 'id': 'call_fhfas8Oy7jE8i9brDiPEGtmh', 'type': 'tool_call'}]
2024-10-30 17:45:43,864 - INFO - Structured Response: {'node': 'agent', 'message': {'content': '', 'type': 'AIMessage', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_fhfas8Oy7jE8i9brDiPEGtmh', 'function': {'arguments': '{"query":"types of streaming in LangGraph"}', 'name': 'retrieve_docs'}, 'type': 'function'}], 'type': 'AIMessage'}}}
2024-10-30 17:45:44,039 - DEBUG - Starting component PersistentLocalHnswSegment
2024-10-30 17:45:44,055 - DEBUG - Raw Message from LangChain: content='of the trade-off between timeliness and accuracy\\\\nearly estimates are based on incomplete data\\\\nVery few statistical revisions arise as a result of \\\\"errors\\\\" in the popular sense of the word."}]\', name=\'tavily_search_results_json\', tool_call_id=\'call_nvB1wQyQuNeTrOXQZnEtgNDZ\')]}}\n\nWe can see the results benchmarked against GPT-4o and Llama-3-70b using Custom agent (as shown here) and ReAct.\n\nThe local custom agent performs well in terms of tool calling reliability: it follows the expected reasoning traces.\nHowever, the answer accuracy performance lags the larger models with custom agent implementations.\nComments\n\nEvaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.\nComments\n\nHome\n\nü¶úüï∏Ô∏èLangGraph¬∂\n\n‚ö° Building language agents as graphs ‚ö°\n\nNote\nLooking for the JS version? Click here (JS docs).\n\nOverview¬∂\nLangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\nTo learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available for free here.\nKey Features¬∂\n\nCycles and Branching: Implement loops and conditionals in your apps.\nPersistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\nHuman-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.\nStreaming Support: Stream outputs as they are produced by each node (including token streaming).\nIntegration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\n\nInstallation¬∂\npip install -U langgraph\n\nExample¬∂\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\nLet\'s take a look at a simple example of an agent that can use a search tool.\npip install langchain-anthropic\n\nexport ANTHROPIC_API_KEY=sk-...\n\nOptionally, we can set up LangSmith for best-in-class observability.\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    """Call to surf the web."""\n    # This is a placeholder, but don\'t tell the LLM that...\n    if "sf" in query.lower() or "san francisco" in query.lower():\n        return "It\'s 60 degrees and foggy."\n    return "It\'s 90 degrees and sunny."\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal["tools", END]:\n    messages = state[\'messages\']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the "tools" node\n    if last_message.tool_calls:\n        return "tools"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state[\'messages\']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {"messages": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node("agent", call_model)\nworkflow.add_node("tools", tool_node)\n\nAPI Reference:\nRunnableConfig\n\nexample_answer = await gen_answer(\n    {"messages": [HumanMessage(content=question["messages"][0].content)]}\n)\nexample_answer["messages"][-1].content\n\n\'Studying the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework involves analyzing how these large language models affect information retrieval and generation tasks within the RAG framework. The introduction of large language models with extensive context windows, such as Google Gemini 1.5 Pro with a record 1 million token context window, has sparked discussions in the AI community about the potential implications for RAG. While there are concerns about the negative impact of supermassive context windows on RAG, there is also a recognition of the benefits they bring, enabling more use cases and enhancing performance in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) combines the generative capabilities of transformer architectures with dynamic information retrieval, allowing large language models to access and integrate relevant external knowledge during text generation, leading to more accurate and credible outputs. RAG has been identified as a valuable solution to address challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, lack of transparency, and untraceable reasoning processes. By incorporating external databases, RAG improves the consistency and coherence of generated content, especially in conversational question answering tasks. RAG has also been recognized as a powerful tool for large language models to efficiently process overly lengthy contexts, with recent LLMs like Gemini-1.5 and GPT-4 showcasing exceptional capabilities in understanding long contexts directly. There is ongoing research and benchmarking to compare the strengths of RAG and long-context LLMs, aiming to leverage the advantages of both approaches for enhanced performance in information retrieval and generation tasks within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25\\n[4]: https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/\\n[5]: https://www.deepset.ai/blog/long-context-llms-rag\\n[6]: https://arxiv.org/abs/2312.10997\\n[7]: https://arxiv.org/abs/2409.13385\\n[8]: https://irisagent.com/blog/enhancing-large-language-models-a-deep-dive-into-rag-llm-technology/\\n[9]: https://arxiv.org/abs/2407.16833\'\n\nConstruct the Interview Graph¬∂\nNow that we\'ve defined the editor and domain expert, we can compose them in a graph.\nmax_num_turns = 5\nfrom langgraph.pregel import RetryPolicy\n\ndef route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):\n    messages = state["messages"]\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n    if num_responses >= max_num_turns:\n        return END\n    last_question = messages[-2]\n    if last_question.content.endswith("Thank you so much for your help!"):\n        return END\n    return "ask_question"\n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))\nbuilder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))\nbuilder.add_conditional_edges("answer_question", route_messages)\nbuilder.add_edge("ask_question", "answer_question")\n\nbuilder.add_edge(START, "ask_question")\ninterview_graph = builder.compile(checkpointer=False).with_config(\n    run_name="Conduct Interviews"\n)\n\nfrom IPython.display import Image, display' name='retrieve_docs' id='66eefd53-586c-48eb-a96d-bf36072e0146' tool_call_id='call_fhfas8Oy7jE8i9brDiPEGtmh'
2024-10-30 17:45:44,362 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-30 17:47:26,459 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': 'Hello! How can I assist you today?', 'role': 'assistant'}, {'content': 'what different types of streaming are in langgraph?', 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': 'call_fhfas8Oy7jE8i9brDiPEGtmh', 'function': {'name': 'retrieve_docs', 'arguments': '{"query": "types of streaming in LangGraph"}'}}]}, {'content': 'of the trade-off between timeliness and accuracy\\\\nearly estimates are based on incomplete data\\\\nVery few statistical revisions arise as a result of \\\\"errors\\\\" in the popular sense of the word."}]\', name=\'tavily_search_results_json\', tool_call_id=\'call_nvB1wQyQuNeTrOXQZnEtgNDZ\')]}}\n\nWe can see the results benchmarked against GPT-4o and Llama-3-70b using Custom agent (as shown here) and ReAct.\n\nThe local custom agent performs well in terms of tool calling reliability: it follows the expected reasoning traces.\nHowever, the answer accuracy performance lags the larger models with custom agent implementations.\nComments\n\nEvaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.\nComments\n\nHome\n\nü¶úüï∏Ô∏èLangGraph¬∂\n\n‚ö° Building language agents as graphs ‚ö°\n\nNote\nLooking for the JS version? Click here (JS docs).\n\nOverview¬∂\nLangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\nTo learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available for free here.\nKey Features¬∂\n\nCycles and Branching: Implement loops and conditionals in your apps.\nPersistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\nHuman-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.\nStreaming Support: Stream outputs as they are produced by each node (including token streaming).\nIntegration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\n\nInstallation¬∂\npip install -U langgraph\n\nExample¬∂\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\nLet\'s take a look at a simple example of an agent that can use a search tool.\npip install langchain-anthropic\n\nexport ANTHROPIC_API_KEY=sk-...\n\nOptionally, we can set up LangSmith for best-in-class observability.\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    """Call to surf the web."""\n    # This is a placeholder, but don\'t tell the LLM that...\n    if "sf" in query.lower() or "san francisco" in query.lower():\n        return "It\'s 60 degrees and foggy."\n    return "It\'s 90 degrees and sunny."\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal["tools", END]:\n    messages = state[\'messages\']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the "tools" node\n    if last_message.tool_calls:\n        return "tools"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state[\'messages\']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {"messages": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node("agent", call_model)\nworkflow.add_node("tools", tool_node)\n\nAPI Reference:\nRunnableConfig\n\nexample_answer = await gen_answer(\n    {"messages": [HumanMessage(content=question["messages"][0].content)]}\n)\nexample_answer["messages"][-1].content\n\n\'Studying the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework involves analyzing how these large language models affect information retrieval and generation tasks within the RAG framework. The introduction of large language models with extensive context windows, such as Google Gemini 1.5 Pro with a record 1 million token context window, has sparked discussions in the AI community about the potential implications for RAG. While there are concerns about the negative impact of supermassive context windows on RAG, there is also a recognition of the benefits they bring, enabling more use cases and enhancing performance in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) combines the generative capabilities of transformer architectures with dynamic information retrieval, allowing large language models to access and integrate relevant external knowledge during text generation, leading to more accurate and credible outputs. RAG has been identified as a valuable solution to address challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, lack of transparency, and untraceable reasoning processes. By incorporating external databases, RAG improves the consistency and coherence of generated content, especially in conversational question answering tasks. RAG has also been recognized as a powerful tool for large language models to efficiently process overly lengthy contexts, with recent LLMs like Gemini-1.5 and GPT-4 showcasing exceptional capabilities in understanding long contexts directly. There is ongoing research and benchmarking to compare the strengths of RAG and long-context LLMs, aiming to leverage the advantages of both approaches for enhanced performance in information retrieval and generation tasks within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25\\n[4]: https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/\\n[5]: https://www.deepset.ai/blog/long-context-llms-rag\\n[6]: https://arxiv.org/abs/2312.10997\\n[7]: https://arxiv.org/abs/2409.13385\\n[8]: https://irisagent.com/blog/enhancing-large-language-models-a-deep-dive-into-rag-llm-technology/\\n[9]: https://arxiv.org/abs/2407.16833\'\n\nConstruct the Interview Graph¬∂\nNow that we\'ve defined the editor and domain expert, we can compose them in a graph.\nmax_num_turns = 5\nfrom langgraph.pregel import RetryPolicy\n\ndef route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):\n    messages = state["messages"]\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n    if num_responses >= max_num_turns:\n        return END\n    last_question = messages[-2]\n    if last_question.content.endswith("Thank you so much for your help!"):\n        return END\n    return "ask_question"\n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))\nbuilder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))\nbuilder.add_conditional_edges("answer_question", route_messages)\nbuilder.add_edge("ask_question", "answer_question")\n\nbuilder.add_edge(START, "ask_question")\ninterview_graph = builder.compile(checkpointer=False).with_config(\n    run_name="Conduct Interviews"\n)\n\nfrom IPython.display import Image, display', 'role': 'tool', 'tool_call_id': 'call_fhfas8Oy7jE8i9brDiPEGtmh'}, {'content': 'hi', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:47:26,461 - DEBUG - close.started
2024-10-30 17:47:26,462 - DEBUG - close.complete
2024-10-30 17:47:26,463 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:47:26,597 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a45bfe20>
2024-10-30 17:47:26,597 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd1c00a3040> server_hostname='api.openai.com' timeout=None
2024-10-30 17:47:26,653 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a4595a50>
2024-10-30 17:47:26,653 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:47:26,654 - DEBUG - send_request_headers.complete
2024-10-30 17:47:26,654 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:47:26,655 - DEBUG - send_request_body.complete
2024-10-30 17:47:26,655 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:47:27,395 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:47:27 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'565'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'447652'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'312ms'), (b'x-request-id', b'req_9d4d344152e7169313abb54d22a69cf1'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8dae9a943c8ea4e2-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:47:27,396 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:47:27,396 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:47:27,397 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:47:27,627 - DEBUG - receive_response_body.complete
2024-10-30 17:47:27,627 - DEBUG - response_closed.started
2024-10-30 17:47:27,628 - DEBUG - response_closed.complete
2024-10-30 17:47:27,631 - DEBUG - Raw Message from LangChain: content='Hello! How can I assist you today?' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0125-preview'} id='run-fe8c250d-ad5d-4535-840a-ebef2288935e-0'
2024-10-30 17:47:27,631 - INFO - Structured Response: {'node': 'agent', 'message': {'content': 'Hello! How can I assist you today?', 'type': 'AIMessage', 'additional_kwargs': {'type': 'AIMessage'}}}
2024-10-30 17:47:42,813 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': 'call_fhfas8Oy7jE8i9brDiPEGtmh', 'function': {'name': 'retrieve_docs', 'arguments': '{"query": "types of streaming in LangGraph"}'}}]}, {'content': 'of the trade-off between timeliness and accuracy\\\\nearly estimates are based on incomplete data\\\\nVery few statistical revisions arise as a result of \\\\"errors\\\\" in the popular sense of the word."}]\', name=\'tavily_search_results_json\', tool_call_id=\'call_nvB1wQyQuNeTrOXQZnEtgNDZ\')]}}\n\nWe can see the results benchmarked against GPT-4o and Llama-3-70b using Custom agent (as shown here) and ReAct.\n\nThe local custom agent performs well in terms of tool calling reliability: it follows the expected reasoning traces.\nHowever, the answer accuracy performance lags the larger models with custom agent implementations.\nComments\n\nEvaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.\nComments\n\nHome\n\nü¶úüï∏Ô∏èLangGraph¬∂\n\n‚ö° Building language agents as graphs ‚ö°\n\nNote\nLooking for the JS version? Click here (JS docs).\n\nOverview¬∂\nLangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\nTo learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available for free here.\nKey Features¬∂\n\nCycles and Branching: Implement loops and conditionals in your apps.\nPersistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\nHuman-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.\nStreaming Support: Stream outputs as they are produced by each node (including token streaming).\nIntegration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\n\nInstallation¬∂\npip install -U langgraph\n\nExample¬∂\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\nLet\'s take a look at a simple example of an agent that can use a search tool.\npip install langchain-anthropic\n\nexport ANTHROPIC_API_KEY=sk-...\n\nOptionally, we can set up LangSmith for best-in-class observability.\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    """Call to surf the web."""\n    # This is a placeholder, but don\'t tell the LLM that...\n    if "sf" in query.lower() or "san francisco" in query.lower():\n        return "It\'s 60 degrees and foggy."\n    return "It\'s 90 degrees and sunny."\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal["tools", END]:\n    messages = state[\'messages\']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the "tools" node\n    if last_message.tool_calls:\n        return "tools"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state[\'messages\']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {"messages": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node("agent", call_model)\nworkflow.add_node("tools", tool_node)\n\nAPI Reference:\nRunnableConfig\n\nexample_answer = await gen_answer(\n    {"messages": [HumanMessage(content=question["messages"][0].content)]}\n)\nexample_answer["messages"][-1].content\n\n\'Studying the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework involves analyzing how these large language models affect information retrieval and generation tasks within the RAG framework. The introduction of large language models with extensive context windows, such as Google Gemini 1.5 Pro with a record 1 million token context window, has sparked discussions in the AI community about the potential implications for RAG. While there are concerns about the negative impact of supermassive context windows on RAG, there is also a recognition of the benefits they bring, enabling more use cases and enhancing performance in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) combines the generative capabilities of transformer architectures with dynamic information retrieval, allowing large language models to access and integrate relevant external knowledge during text generation, leading to more accurate and credible outputs. RAG has been identified as a valuable solution to address challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, lack of transparency, and untraceable reasoning processes. By incorporating external databases, RAG improves the consistency and coherence of generated content, especially in conversational question answering tasks. RAG has also been recognized as a powerful tool for large language models to efficiently process overly lengthy contexts, with recent LLMs like Gemini-1.5 and GPT-4 showcasing exceptional capabilities in understanding long contexts directly. There is ongoing research and benchmarking to compare the strengths of RAG and long-context LLMs, aiming to leverage the advantages of both approaches for enhanced performance in information retrieval and generation tasks within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25\\n[4]: https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/\\n[5]: https://www.deepset.ai/blog/long-context-llms-rag\\n[6]: https://arxiv.org/abs/2312.10997\\n[7]: https://arxiv.org/abs/2409.13385\\n[8]: https://irisagent.com/blog/enhancing-large-language-models-a-deep-dive-into-rag-llm-technology/\\n[9]: https://arxiv.org/abs/2407.16833\'\n\nConstruct the Interview Graph¬∂\nNow that we\'ve defined the editor and domain expert, we can compose them in a graph.\nmax_num_turns = 5\nfrom langgraph.pregel import RetryPolicy\n\ndef route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):\n    messages = state["messages"]\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n    if num_responses >= max_num_turns:\n        return END\n    last_question = messages[-2]\n    if last_question.content.endswith("Thank you so much for your help!"):\n        return END\n    return "ask_question"\n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))\nbuilder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))\nbuilder.add_conditional_edges("answer_question", route_messages)\nbuilder.add_edge("ask_question", "answer_question")\n\nbuilder.add_edge(START, "ask_question")\ninterview_graph = builder.compile(checkpointer=False).with_config(\n    run_name="Conduct Interviews"\n)\n\nfrom IPython.display import Image, display', 'role': 'tool', 'tool_call_id': 'call_fhfas8Oy7jE8i9brDiPEGtmh'}, {'content': 'hi', 'role': 'user'}, {'content': 'Hello! How can I assist you today?', 'role': 'assistant'}, {'content': 'what are the different types of streaming in langgraph?', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:47:42,815 - DEBUG - close.started
2024-10-30 17:47:42,815 - DEBUG - close.complete
2024-10-30 17:47:42,816 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:47:42,911 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a4595180>
2024-10-30 17:47:42,912 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd1c00a3040> server_hostname='api.openai.com' timeout=None
2024-10-30 17:47:42,966 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a45bfee0>
2024-10-30 17:47:42,967 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:47:42,968 - DEBUG - send_request_headers.complete
2024-10-30 17:47:42,968 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:47:42,969 - DEBUG - send_request_body.complete
2024-10-30 17:47:42,969 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:47:43,859 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:47:43 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'698'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'447652'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'313ms'), (b'x-request-id', b'req_05d76929e6b2fedb61add20bd51cc5cb'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8dae9afa1a1731d8-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:47:43,860 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:47:43,860 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:47:43,861 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:47:52,226 - DEBUG - receive_response_body.complete
2024-10-30 17:47:52,226 - DEBUG - response_closed.started
2024-10-30 17:47:52,226 - DEBUG - response_closed.complete
2024-10-30 17:47:52,231 - DEBUG - Raw Message from LangChain: content="In LangGraph, streaming support is designed to allow outputs to be streamed as they are produced by each node. This includes token streaming, which is particularly useful for applications that require real-time processing or incremental output generation. Here's a brief overview of the types of streaming supported in LangGraph:\n\n1. **Output Streaming**: This type of streaming allows the outputs of nodes within a LangGraph application to be streamed to subsequent nodes or to the end-user as soon as they are produced. This is useful for applications where latency is critical, and it's beneficial to start processing or displaying results without waiting for the entire computation to complete.\n\n2. **Token Streaming**: Specifically designed for language models, token streaming involves streaming the tokens (words or pieces of text) generated by a language model as they are produced. This is particularly useful for chat applications, real-time translation, or any scenario where immediate feedback or output is necessary.\n\nThese streaming capabilities are part of LangGraph's support for building stateful, multi-actor applications with large language models (LLMs), enhancing real-time interaction, responsiveness, and the overall user experience in applications built with LangGraph." additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0125-preview'} id='run-b05b4302-09ba-4fac-8566-34e39f8c5e6e-0'
2024-10-30 17:47:52,232 - INFO - Structured Response: {'node': 'agent', 'message': {'content': "In LangGraph, streaming support is designed to allow outputs to be streamed as they are produced by each node. This includes token streaming, which is particularly useful for applications that require real-time processing or incremental output generation. Here's a brief overview of the types of streaming supported in LangGraph:\n\n1. **Output Streaming**: This type of streaming allows the outputs of nodes within a LangGraph application to be streamed to subsequent nodes or to the end-user as soon as they are produced. This is useful for applications where latency is critical, and it's beneficial to start processing or displaying results without waiting for the entire computation to complete.\n\n2. **Token Streaming**: Specifically designed for language models, token streaming involves streaming the tokens (words or pieces of text) generated by a language model as they are produced. This is particularly useful for chat applications, real-time translation, or any scenario where immediate feedback or output is necessary.\n\nThese streaming capabilities are part of LangGraph's support for building stateful, multi-actor applications with large language models (LLMs), enhancing real-time interaction, responsiveness, and the overall user experience in applications built with LangGraph.", 'type': 'AIMessage', 'additional_kwargs': {'type': 'AIMessage'}}}
2024-10-30 17:48:17,762 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': 'hi', 'role': 'user'}, {'content': 'Hello! How can I assist you today?', 'role': 'assistant'}, {'content': 'what are the different types of streaming in langgraph?', 'role': 'user'}, {'content': "In LangGraph, streaming support is designed to allow outputs to be streamed as they are produced by each node. This includes token streaming, which is particularly useful for applications that require real-time processing or incremental output generation. Here's a brief overview of the types of streaming supported in LangGraph:\n\n1. **Output Streaming**: This type of streaming allows the outputs of nodes within a LangGraph application to be streamed to subsequent nodes or to the end-user as soon as they are produced. This is useful for applications where latency is critical, and it's beneficial to start processing or displaying results without waiting for the entire computation to complete.\n\n2. **Token Streaming**: Specifically designed for language models, token streaming involves streaming the tokens (words or pieces of text) generated by a language model as they are produced. This is particularly useful for chat applications, real-time translation, or any scenario where immediate feedback or output is necessary.\n\nThese streaming capabilities are part of LangGraph's support for building stateful, multi-actor applications with large language models (LLMs), enhancing real-time interaction, responsiveness, and the overall user experience in applications built with LangGraph.", 'role': 'assistant'}, {'content': 'search the store for answer', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:48:17,764 - DEBUG - close.started
2024-10-30 17:48:17,765 - DEBUG - close.complete
2024-10-30 17:48:17,765 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:48:17,860 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a45c8f10>
2024-10-30 17:48:17,861 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7fd1c00a3040> server_hostname='api.openai.com' timeout=None
2024-10-30 17:48:17,914 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7fd1a45963b0>
2024-10-30 17:48:17,914 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:48:17,915 - DEBUG - send_request_headers.complete
2024-10-30 17:48:17,915 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:48:17,916 - DEBUG - send_request_body.complete
2024-10-30 17:48:17,916 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:48:19,264 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:48:19 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'1110'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'449601'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'53ms'), (b'x-request-id', b'req_145da25f4ec8bed9702a46de5eb81201'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8dae9bd4799d9ab3-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:48:19,265 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:48:19,265 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:48:19,265 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:48:19,272 - DEBUG - receive_response_body.complete
2024-10-30 17:48:19,272 - DEBUG - response_closed.started
2024-10-30 17:48:19,272 - DEBUG - response_closed.complete
2024-10-30 17:48:19,275 - DEBUG - Raw Message from LangChain: content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_PO8cGI2biMR9zdQ95M06oUTZ', 'function': {'arguments': '{"query": "types of streaming in LangGraph"}', 'name': 'retrieve_docs'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-0125-preview'} id='run-0ede633c-05ce-42fb-93ef-2493b05be512-0' tool_calls=[{'name': 'retrieve_docs', 'args': {'query': 'types of streaming in LangGraph'}, 'id': 'call_PO8cGI2biMR9zdQ95M06oUTZ', 'type': 'tool_call'}]
2024-10-30 17:48:19,275 - INFO - Structured Response: {'node': 'agent', 'message': {'content': '', 'type': 'AIMessage', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_PO8cGI2biMR9zdQ95M06oUTZ', 'function': {'arguments': '{"query": "types of streaming in LangGraph"}', 'name': 'retrieve_docs'}, 'type': 'function'}], 'type': 'AIMessage'}}}
2024-10-30 17:48:19,363 - DEBUG - Raw Message from LangChain: content='of the trade-off between timeliness and accuracy\\\\nearly estimates are based on incomplete data\\\\nVery few statistical revisions arise as a result of \\\\"errors\\\\" in the popular sense of the word."}]\', name=\'tavily_search_results_json\', tool_call_id=\'call_nvB1wQyQuNeTrOXQZnEtgNDZ\')]}}\n\nWe can see the results benchmarked against GPT-4o and Llama-3-70b using Custom agent (as shown here) and ReAct.\n\nThe local custom agent performs well in terms of tool calling reliability: it follows the expected reasoning traces.\nHowever, the answer accuracy performance lags the larger models with custom agent implementations.\nComments\n\nEvaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.\nComments\n\nHome\n\nü¶úüï∏Ô∏èLangGraph¬∂\n\n‚ö° Building language agents as graphs ‚ö°\n\nNote\nLooking for the JS version? Click here (JS docs).\n\nOverview¬∂\nLangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\nTo learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available for free here.\nKey Features¬∂\n\nCycles and Branching: Implement loops and conditionals in your apps.\nPersistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\nHuman-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.\nStreaming Support: Stream outputs as they are produced by each node (including token streaming).\nIntegration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\n\nInstallation¬∂\npip install -U langgraph\n\nExample¬∂\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\nLet\'s take a look at a simple example of an agent that can use a search tool.\npip install langchain-anthropic\n\nexport ANTHROPIC_API_KEY=sk-...\n\nOptionally, we can set up LangSmith for best-in-class observability.\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    """Call to surf the web."""\n    # This is a placeholder, but don\'t tell the LLM that...\n    if "sf" in query.lower() or "san francisco" in query.lower():\n        return "It\'s 60 degrees and foggy."\n    return "It\'s 90 degrees and sunny."\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal["tools", END]:\n    messages = state[\'messages\']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the "tools" node\n    if last_message.tool_calls:\n        return "tools"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state[\'messages\']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {"messages": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node("agent", call_model)\nworkflow.add_node("tools", tool_node)\n\nAPI Reference:\nRunnableConfig\n\nexample_answer = await gen_answer(\n    {"messages": [HumanMessage(content=question["messages"][0].content)]}\n)\nexample_answer["messages"][-1].content\n\n\'Studying the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework involves analyzing how these large language models affect information retrieval and generation tasks within the RAG framework. The introduction of large language models with extensive context windows, such as Google Gemini 1.5 Pro with a record 1 million token context window, has sparked discussions in the AI community about the potential implications for RAG. While there are concerns about the negative impact of supermassive context windows on RAG, there is also a recognition of the benefits they bring, enabling more use cases and enhancing performance in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) combines the generative capabilities of transformer architectures with dynamic information retrieval, allowing large language models to access and integrate relevant external knowledge during text generation, leading to more accurate and credible outputs. RAG has been identified as a valuable solution to address challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, lack of transparency, and untraceable reasoning processes. By incorporating external databases, RAG improves the consistency and coherence of generated content, especially in conversational question answering tasks. RAG has also been recognized as a powerful tool for large language models to efficiently process overly lengthy contexts, with recent LLMs like Gemini-1.5 and GPT-4 showcasing exceptional capabilities in understanding long contexts directly. There is ongoing research and benchmarking to compare the strengths of RAG and long-context LLMs, aiming to leverage the advantages of both approaches for enhanced performance in information retrieval and generation tasks within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25\\n[4]: https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/\\n[5]: https://www.deepset.ai/blog/long-context-llms-rag\\n[6]: https://arxiv.org/abs/2312.10997\\n[7]: https://arxiv.org/abs/2409.13385\\n[8]: https://irisagent.com/blog/enhancing-large-language-models-a-deep-dive-into-rag-llm-technology/\\n[9]: https://arxiv.org/abs/2407.16833\'\n\nConstruct the Interview Graph¬∂\nNow that we\'ve defined the editor and domain expert, we can compose them in a graph.\nmax_num_turns = 5\nfrom langgraph.pregel import RetryPolicy\n\ndef route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):\n    messages = state["messages"]\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n    if num_responses >= max_num_turns:\n        return END\n    last_question = messages[-2]\n    if last_question.content.endswith("Thank you so much for your help!"):\n        return END\n    return "ask_question"\n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))\nbuilder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))\nbuilder.add_conditional_edges("answer_question", route_messages)\nbuilder.add_edge("ask_question", "answer_question")\n\nbuilder.add_edge(START, "ask_question")\ninterview_graph = builder.compile(checkpointer=False).with_config(\n    run_name="Conduct Interviews"\n)\n\nfrom IPython.display import Image, display' name='retrieve_docs' id='f18552ec-cb38-4df8-a855-02a99b928d60' tool_call_id='call_PO8cGI2biMR9zdQ95M06oUTZ'
2024-10-30 17:50:47,298 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors.index.json HTTP/11" 404 0
2024-10-30 17:50:47,395 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors HTTP/11" 302 0
2024-10-30 17:50:54,950 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': 'what are the different types of streaming on langgraph agents?', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:50:54,952 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:50:55,091 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f8cd3013490>
2024-10-30 17:50:55,092 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f8cd46b8440> server_hostname='api.openai.com' timeout=None
2024-10-30 17:50:55,146 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f8cd3012f20>
2024-10-30 17:50:55,147 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:50:55,148 - DEBUG - send_request_headers.complete
2024-10-30 17:50:55,148 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:50:55,149 - DEBUG - send_request_body.complete
2024-10-30 17:50:55,149 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:50:55,952 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:50:55 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'624'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'449942'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'7ms'), (b'x-request-id', b'req_0cdec623d718045465236d5d536118cf'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=mGTvZuZ7mYyDjy8UrD1w0hF6JMHLMfzDBrIaAlm4NR4-1730325055-1.0.1.1-CWAMYEypCDGPncpSpRlxK9yANO0AnvAL8oI77KV9uUeoQSBtlXN1P6gK9DmSuiHiejVyYEsszJZSMabQh4S3Bg; path=/; expires=Wed, 30-Oct-24 22:20:55 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=BQHQ.4Dd85jhifk0v3oUfFEAYs_y0WsrhbJf7UnccDY-1730325055977-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8dae9fab2eec7458-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:50:55,956 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:50:55,957 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:50:55,959 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:50:57,393 - DEBUG - receive_response_body.complete
2024-10-30 17:50:57,393 - DEBUG - response_closed.started
2024-10-30 17:50:57,394 - DEBUG - response_closed.complete
2024-10-30 17:50:57,400 - DEBUG - Raw Message from LangChain: content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8pNpHRshEJCQdNQ4KE9UGH5g', 'function': {'arguments': '{"query":"types of streaming on LangGraph agents"}', 'name': 'retrieve_docs'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-0125-preview'} id='run-23484db9-e07b-4b6b-b048-4a1adc67f043-0' tool_calls=[{'name': 'retrieve_docs', 'args': {'query': 'types of streaming on LangGraph agents'}, 'id': 'call_8pNpHRshEJCQdNQ4KE9UGH5g', 'type': 'tool_call'}]
2024-10-30 17:50:57,400 - INFO - Structured Response: {'node': 'agent', 'message': {'content': '', 'type': 'AIMessage', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_8pNpHRshEJCQdNQ4KE9UGH5g', 'function': {'arguments': '{"query":"types of streaming on LangGraph agents"}', 'name': 'retrieve_docs'}, 'type': 'function'}], 'type': 'AIMessage'}}}
2024-10-30 17:50:57,601 - DEBUG - Starting component PersistentLocalHnswSegment
2024-10-30 17:50:57,631 - DEBUG - Raw Message from LangChain: content='of the trade-off between timeliness and accuracy\\\\nearly estimates are based on incomplete data\\\\nVery few statistical revisions arise as a result of \\\\"errors\\\\" in the popular sense of the word."}]\', name=\'tavily_search_results_json\', tool_call_id=\'call_nvB1wQyQuNeTrOXQZnEtgNDZ\')]}}\n\nAPI Reference:\nRunnableConfig\n\nexample_answer = await gen_answer(\n    {"messages": [HumanMessage(content=question["messages"][0].content)]}\n)\nexample_answer["messages"][-1].content\n\n\'Studying the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework involves analyzing how these large language models affect information retrieval and generation tasks within the RAG framework. The introduction of large language models with extensive context windows, such as Google Gemini 1.5 Pro with a record 1 million token context window, has sparked discussions in the AI community about the potential implications for RAG. While there are concerns about the negative impact of supermassive context windows on RAG, there is also a recognition of the benefits they bring, enabling more use cases and enhancing performance in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) combines the generative capabilities of transformer architectures with dynamic information retrieval, allowing large language models to access and integrate relevant external knowledge during text generation, leading to more accurate and credible outputs. RAG has been identified as a valuable solution to address challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, lack of transparency, and untraceable reasoning processes. By incorporating external databases, RAG improves the consistency and coherence of generated content, especially in conversational question answering tasks. RAG has also been recognized as a powerful tool for large language models to efficiently process overly lengthy contexts, with recent LLMs like Gemini-1.5 and GPT-4 showcasing exceptional capabilities in understanding long contexts directly. There is ongoing research and benchmarking to compare the strengths of RAG and long-context LLMs, aiming to leverage the advantages of both approaches for enhanced performance in information retrieval and generation tasks within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25\\n[4]: https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/\\n[5]: https://www.deepset.ai/blog/long-context-llms-rag\\n[6]: https://arxiv.org/abs/2312.10997\\n[7]: https://arxiv.org/abs/2409.13385\\n[8]: https://irisagent.com/blog/enhancing-large-language-models-a-deep-dive-into-rag-llm-technology/\\n[9]: https://arxiv.org/abs/2407.16833\'\n\nConstruct the Interview Graph¬∂\nNow that we\'ve defined the editor and domain expert, we can compose them in a graph.\nmax_num_turns = 5\nfrom langgraph.pregel import RetryPolicy\n\ndef route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):\n    messages = state["messages"]\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n    if num_responses >= max_num_turns:\n        return END\n    last_question = messages[-2]\n    if last_question.content.endswith("Thank you so much for your help!"):\n        return END\n    return "ask_question"\n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))\nbuilder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))\nbuilder.add_conditional_edges("answer_question", route_messages)\nbuilder.add_edge("ask_question", "answer_question")\n\nbuilder.add_edge(START, "ask_question")\ninterview_graph = builder.compile(checkpointer=False).with_config(\n    run_name="Conduct Interviews"\n)\n\nfrom IPython.display import Image, display\n\nsays Pieter de Pous, program lead for fossil fuel transition at the climate think tank E3G, in a ..."}, {"url": "https://www.wri.org/insights/just-transition-developing-countries-shift-oil-gas", "content": "At the same time insistence from vulnerable countries and others to cut dependence on fossil fuels to avoid catastrophic global warming continues. The transition away from oil and gas to meet global climate goals can offer important environmental, social and economic benefits but also presents significant challenges for many countries."}, {"url": "https://link.springer.com/article/10.1007/s10098-021-02123-x", "content": "The unfolding future is particularly uncertain for the BRICS economies, which, by the year 2030, might respond for 37.7% of the global gross national product, besides representing more than 50% of the actual global economic growth and 40% of the global population. Footnote 6 Similarly, biomass combustion for combined heat and power production is a carbon sink when combined with CCS.Footnote 7 The more stringent the climate targets become, the more urgent the need for near zero-carbon or negative emissions technologies (NET), a niche that fosters bioenergy with CCS (BECCS).\\n How is the transition away from fossil fuels doing, and how will the low-carbon future unfold?\\n2760 Accesses\\n9 Citations\\n1 Altmetric\\nExplore all metrics\\nGraphic abstract\\nAvoid common mistakes on your manuscript.\\n However, besides economic penalty on the carbon-emitting process, CCS has main drawbacks that increase uncertainty and retards deployments: (i) geological sites for carbon storage are not evenly spread geographically and most often are distant from the carbon emission sources; (ii) public concerns on carbon leakages and consequential effects (e.g., induced seismicity); and (iii) lack of a regulatory framework for post-injection liability. Athos da Silveira Ramos, 149, Centro de Tecnologia, E, Ilha do Fund\\u00e3o, 21941-972, Rio de Janeiro, RJ, Brazil\\nOf\\u00e9lia Q. F. Ara\\u00fajo\\u00a0&\\u00a0Jos\\u00e9 Luiz de Medeiros\\nYou can also search for this author in\\nPubMed\\u00a0Google Scholar\\nYou can also search for this author in\\nPubMed\\u00a0Google Scholar\\nCorresponding author\\nCorrespondence to\\nOf\\u00e9lia Q. F. Ara\\u00fajo.\\n"}], [{"url": "https://unfccc.int/topics/introduction-to-climate-finance", "content": "The UNFCCC website includes a climate finance data portal with helpful explanations, graphics and figures for better understanding the climate finance process and as a gateway to information on activities funded in developing countries to implement climate action. The finance portal comprises three modules, each of which includes information ..."}, {"url": "https://www.worldbank.org/en/news/factsheet/2022/09/30/10-things-you-should-know-about-the-world-bank-group-s-climate-finance", "content": "Did you know\\u2026\\nRELATED\\nWorld Bank - Climate Change\\nClimate Stories: How Countries and Communities Are Shaping a Sustainable Future\\nClimate Explainer Series\\nThis site uses cookies to optimize functionality and give you the best possible experience. 10 Things You Should Know About the World Bank Group\\u2019s Climate Finance\\nPhoto: World Bank\\nFinancing transformative climate action is vital for development and to support the poorest people who are most affected by climate change. With 189 member countries, staff from more than 170 countries, and offices in over 130 locations, the World Bank Group is a unique global partnership: five institutions working for sustainable solutions that reduce poverty and build shared prosperity in developing countries.\\n We provide a wide array of financial products and technical assistance, and we help countries share and apply innovative knowledge and solutions to the challenges they face.\\n Data and research help us understand these challenges and set priorities, share knowledge of what works, and measure progress.\\n"}, {"url": "https://news.un.org/en/story/2021/06/1094762",\n\nEfficiency and Demand\\nCarbon Capture, Utilisation and Storage\\nDecarbonisation Enablers\\nGlobal Energy Transitions Stocktake\\nCritical Minerals\\nRussia\'s War on Ukraine\\nClimate Change\\nGlobal Energy Crisis\\nInvestment\\nSaving Energy\\nEnergy Security\\nNet Zero Emissions\\nEnergy Efficiency\\nData explorers\\nUnderstand and manipulate data with easy to use explorers and trackers\\nData sets\\nFree and paid data sets from across the energy system available for download\\nPolicies database\\nPast, existing or planned government policies and measures\\nChart Library\\nAccess every chart published across all IEA reports and analysis\\nWorld Energy Outlook 2023\\nFlagship report \\u2014 October 2023\\nOil Market Report - December 2023\\nFuel report \\u2014 December 2023\\nEnergy Efficiency 2023\\nFuel report \\u2014 November 2023\\nNet Zero Roadmap: The rapid decarbonisation of the power system is critical for the success of the clean energy transition, since power generation accounts for 40% of energy-related CO2 emissions and electricity is increasingly being used to meet energy demand in key sectors of the economy.\\n The International Energy Agency\\u2019s latest and most comprehensive assessment of clean energy technology progress worldwide shows that a step change in action and ambition is needed across all energy technologies and sectors to keep the goal of net zero emissions by 2050 within reach.\\n Progress on clean energy innovation will be crucial to help develop and deploy the full range of clean energy technologies needed to decarbonise the sectors, in particular those where emissions are the most challenging to reduce, such as aviation, shipping and heavy industry.\\n In transport, stronger policies are needed to encourage shifts to using low-carbon modes of transport, greater energy efficiency measures, and the building out of infrastructure to support zero emission vehicles, as well as the development and uptake of those vehicle in long-distance transport.\\n"}], [{"url": "https://www.iucn.org/our-work/topic/nature-based-solutions-climate", "content": "Enhancing Nature-Based Solutions in Kosovo\\nPublication\\n|\\n2023\\nNature-based Solutions for corporate climate targets\\nNews\\n|\\n09 Nov, 2023\\nReSea Project Launched to Strengthen Coastal Communities in Kenya\\nBlog\\n|\\n01 Nov, 2023\\nTREPA project to plant over 18,000 ha of native species during 2023-2024 tree planting season\\u2026\\nSign up for an IUCN newsletter\\nFeatured bottom second Menus\\nSECRETARIAT\\nCOMMISSIONS\\nTHEMES\\nREGIONS\\nContact\\nHeadquarters\\nRue Mauverney 28\\n1196 Gland\\nSwitzerland\\n+41 22 9990000\\n+41 22 9990002(Fax)\\nFollow Us\\n\\u00a9IUCN, International Union for Conservation of Nature and Natural Resources Nature-based solutions can address climate change in three ways:\\nHeading\\n30%\\nof the global mitigation required by 2030/2050 to achieve the 1.5/2\\u00b0C temperature rise goal agreed to under the Paris Agreement\\nRead more\\nHeading\\n5 GtCO2e\\n5 GtCO2e\\nNature-based Solutions could deliver emission reductions\\nand removals of at least 5 GtCO2e per year by 2030 (of a maximum estimate of 11.7 GtCO2e per year).\\n Learn more\\nHeading\\nUSD 393 Billion\\nwhich can reduce the intensity of climate hazards by 26%\\nRead more\\nIUCN\'s work on NbS for climate\\nIUCN works to advance practical nature-based solutions for both climate mitigation and adaptation, centred on the better conservation, management and restoration of the world\\u2019s ecosystems. IUCN Issues Brief: Ensuring effective Nature-based Solutions\\nAccelerating investment in Nature-based Climate Solutions\\nIUCN supports the acceleration of financing for nature-based solutions for climate change through multiple grant mechanisms, including the Global EbA Fund, the Blue Natural Capital Financing Facility, the Subnational Climate Finance initiative, and the Nature+ Accelerator Fund, which collectively represent 200 million USD in available funding for NbS. Current economic valuation research estimates that an investment of\n\nreducing carbon intensity in a short timeframe (one to two decades). The transition driver is limiting global warming caused by greenhouse gases, majorly emitted by fossil fuels and, to a lesser extent, land-use changes."}, {"url": "https://link.springer.com/article/10.1007/s10098-021-02123-x", "content": "The transition from a fossil-based to a low-carbon economy (based on renewable energies and hydrogen as energy carrier) targets reducing carbon intensity in a short timeframe (one to two decades). The transition driver is limiting global warming caused by greenhouse gases, majorly emitted by fossil fuels and, to a lesser extent, land-use changes."}, {"url": "https://www.anl.gov/sites/www/files/2024-01/Net-Zero-World-Fossil-Transition-Report_FINAL_1-8-2024.pdf", "content": "support to inform community fossil fuel transitions. As a first step, this analysis examines the decision-making processes of fossil fuel transitions in several communities across two countries: the United States and Chile. The goal is a framework that lifts out key decision-making criteria and learnings from communities that have undergone fossil"}], [{"url": "https://www.un.org/en/our-work/support-sustainable-development-and-climate-action", "content": "MDGs \\u2014 Close to 40 per cent of the population of the developing world was ... climate action; life ... a critical role in supporting countries in their efforts to implement the 2030 Agenda by ..."}, {"url": "https://www.worldbank.org/en/topic/climatechange/overview", "content": "Sustainable Development Series\\nThis series offers insights into innovative and state-of-the-art solutions that can guide countries to build more inclusive and sustainable economies that are resilient in the face of pandemics, climate change and other ...\\nIDA and Climate Change\\nIDA helps the poorest nations adapt to climate change by building their resilience to disasters, and promoting sustainable development to minimize their vulnerability.\\n Carbon Pricing Dashboard\\nThis interactive dashboard provides an up-to-date overview of carbon pricing initiatives around the world and allows users to navigate through the visuals and data of the annual State and Trends of Carbon Pricing report ...\\nAdditional Resources\\nRelated\\nContact\\nThis site uses cookies to optimize functionality and give you the best possible experience. Forest Carbon Partnership Facility\\nThe Forest Carbon Partnership Facility is focused on reducing emissions from deforestation and forest degradation, forest carbon stock conservation, the sustainable management of forests, and the enhancement of forest ...\\nBioCarbon Fund Initiative for Sustainable Forest Landscapes\\nThe BioCarbon Fund Initiative for Sustainable Forest Landscapes is focused on reducing emissions from the land sector through smarter land use planning, policies, and practices.\\n The Carbon Pricing Leadership Coalition brings together leaders from across government, the private sector and civil society to share experience working with carbon pricing and to expand the evidence base for the most ...\\nIFC Climate Business\\nIFC invests in the private sector in clean energy, sustainable cities, climate-smart agriculture, energy efficiency, green buildings and green finance.\\n Oct 12, 2023\\nRELATED\\nMULTIMEDIA\\nFinancing the Climate Transition: Building the Green, Inclusive, Resilient Economies of the Future\\nAROUND THE BANK GROUP\\nFind out what the Bank Group\'s branches are doing on climate change.\\n"}, {"url": "https://climatepromise.undp.org/news-and-stories/NDCs-nationally-determined-contributions-climate-change-what-you-need-to-know", "content": "Summary. Nationally Determined Contributions, or NDCs, are countries\' self-defined national climate pledges under the Paris Agreement, detailing what they will do to help meet the global goal to pursue 1.5\\u00b0C, adapt to climate impacts and ensure sufficient finance to support these efforts. NDCs represent short- to medium-term plans and are ..."},' name='retrieve_docs' id='89f8fdd4-0777-4d9a-9ab3-bd48557af014' tool_call_id='call_8pNpHRshEJCQdNQ4KE9UGH5g'
2024-10-30 17:50:58,112 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-30 17:52:11,664 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-30 17:52:14,051 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph and LangChain.', 'role': 'system'}, {'content': 'what are the different types of streaming on langgraph agents?', 'role': 'user'}], 'model': 'gpt-4-0125-preview', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_docs', 'description': 'Search through the documentation to find relevant information.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-30 17:52:14,054 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-30 17:52:14,188 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f8ca3ed3b20>
2024-10-30 17:52:14,188 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f8cc426f040> server_hostname='api.openai.com' timeout=None
2024-10-30 17:52:14,251 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f8ca3ed35b0>
2024-10-30 17:52:14,251 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-30 17:52:14,252 - DEBUG - send_request_headers.complete
2024-10-30 17:52:14,252 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-30 17:52:14,253 - DEBUG - send_request_body.complete
2024-10-30 17:52:14,254 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-30 17:52:18,139 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 30 Oct 2024 21:52:18 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'3672'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'450000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'449942'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'7ms'), (b'x-request-id', b'req_4ad8a8ec9b63a31193d6484201b9e6ff'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=gInM2P5nWMVLjii7FX_TAc3nD5JELtRA84utdorxra0-1730325138-1.0.1.1-YmHhX2AMQu0pm8dQiXmEDT5t1NDNGGKZmrSzBC04x8Vpeypb8aFFVbupWRekj2F2fKS0xbMVbMqB4BxWk35MZQ; path=/; expires=Wed, 30-Oct-24 22:22:18 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=EuOylXzmMe2z7IgbocsJkb2XbR3UFsqosqZZGoNExL4-1730325138107-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8daea19979ea6dd3-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-30 17:52:18,141 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-30 17:52:18,141 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-30 17:52:18,141 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-30 17:52:18,181 - DEBUG - receive_response_body.complete
2024-10-30 17:52:18,181 - DEBUG - response_closed.started
2024-10-30 17:52:18,181 - DEBUG - response_closed.complete
2024-10-30 17:52:18,186 - DEBUG - Raw Message from LangChain: content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_qwwxybfYFcCdeJnPbKwKJwpc', 'function': {'arguments': '{"query":"types of streaming on LangGraph agents"}', 'name': 'retrieve_docs'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-0125-preview'} id='run-8551ca5e-bb60-4a66-8aeb-2c53593b2c97-0' tool_calls=[{'name': 'retrieve_docs', 'args': {'query': 'types of streaming on LangGraph agents'}, 'id': 'call_qwwxybfYFcCdeJnPbKwKJwpc', 'type': 'tool_call'}]
2024-10-30 17:52:18,187 - INFO - Structured Response: {'node': 'agent', 'message': {'content': '', 'type': 'AIMessage', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_qwwxybfYFcCdeJnPbKwKJwpc', 'function': {'arguments': '{"query":"types of streaming on LangGraph agents"}', 'name': 'retrieve_docs'}, 'type': 'function'}], 'type': 'AIMessage'}}}
2024-10-30 17:52:18,392 - DEBUG - Starting component PersistentLocalHnswSegment
2024-10-30 17:52:18,411 - DEBUG - Raw Message from LangChain: content='of the trade-off between timeliness and accuracy\\\\nearly estimates are based on incomplete data\\\\nVery few statistical revisions arise as a result of \\\\"errors\\\\" in the popular sense of the word."}]\', name=\'tavily_search_results_json\', tool_call_id=\'call_nvB1wQyQuNeTrOXQZnEtgNDZ\')]}}\n\nAPI Reference:\nRunnableConfig\n\nexample_answer = await gen_answer(\n    {"messages": [HumanMessage(content=question["messages"][0].content)]}\n)\nexample_answer["messages"][-1].content\n\n\'Studying the impact of million-plus token context window language models on the Retrieval-Augmented Generation (RAG) framework involves analyzing how these large language models affect information retrieval and generation tasks within the RAG framework. The introduction of large language models with extensive context windows, such as Google Gemini 1.5 Pro with a record 1 million token context window, has sparked discussions in the AI community about the potential implications for RAG. While there are concerns about the negative impact of supermassive context windows on RAG, there is also a recognition of the benefits they bring, enabling more use cases and enhancing performance in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) combines the generative capabilities of transformer architectures with dynamic information retrieval, allowing large language models to access and integrate relevant external knowledge during text generation, leading to more accurate and credible outputs. RAG has been identified as a valuable solution to address challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, lack of transparency, and untraceable reasoning processes. By incorporating external databases, RAG improves the consistency and coherence of generated content, especially in conversational question answering tasks. RAG has also been recognized as a powerful tool for large language models to efficiently process overly lengthy contexts, with recent LLMs like Gemini-1.5 and GPT-4 showcasing exceptional capabilities in understanding long contexts directly. There is ongoing research and benchmarking to compare the strengths of RAG and long-context LLMs, aiming to leverage the advantages of both approaches for enhanced performance in information retrieval and generation tasks within the RAG framework.\\n\\nCitations:\\n\\n[1]: https://thenewstack.io/do-enormous-llm-context-windows-spell-the-end-of-rag/\\n[2]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[3]: https://medium.com/@amanatulla1606/rag-is-here-to-stay-four-reasons-why-large-context-windows-cant-replace-it-ad112013de25\\n[4]: https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/\\n[5]: https://www.deepset.ai/blog/long-context-llms-rag\\n[6]: https://arxiv.org/abs/2312.10997\\n[7]: https://arxiv.org/abs/2409.13385\\n[8]: https://irisagent.com/blog/enhancing-large-language-models-a-deep-dive-into-rag-llm-technology/\\n[9]: https://arxiv.org/abs/2407.16833\'\n\nConstruct the Interview Graph¬∂\nNow that we\'ve defined the editor and domain expert, we can compose them in a graph.\nmax_num_turns = 5\nfrom langgraph.pregel import RetryPolicy\n\ndef route_messages(state: InterviewState, name: str = "Subject_Matter_Expert"):\n    messages = state["messages"]\n    num_responses = len(\n        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n    )\n    if num_responses >= max_num_turns:\n        return END\n    last_question = messages[-2]\n    if last_question.content.endswith("Thank you so much for your help!"):\n        return END\n    return "ask_question"\n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node("ask_question", generate_question, retry=RetryPolicy(max_attempts=5))\nbuilder.add_node("answer_question", gen_answer, retry=RetryPolicy(max_attempts=5))\nbuilder.add_conditional_edges("answer_question", route_messages)\nbuilder.add_edge("ask_question", "answer_question")\n\nbuilder.add_edge(START, "ask_question")\ninterview_graph = builder.compile(checkpointer=False).with_config(\n    run_name="Conduct Interviews"\n)\n\nfrom IPython.display import Image, display\n\nsays Pieter de Pous, program lead for fossil fuel transition at the climate think tank E3G, in a ..."}, {"url": "https://www.wri.org/insights/just-transition-developing-countries-shift-oil-gas", "content": "At the same time insistence from vulnerable countries and others to cut dependence on fossil fuels to avoid catastrophic global warming continues. The transition away from oil and gas to meet global climate goals can offer important environmental, social and economic benefits but also presents significant challenges for many countries."}, {"url": "https://link.springer.com/article/10.1007/s10098-021-02123-x", "content": "The unfolding future is particularly uncertain for the BRICS economies, which, by the year 2030, might respond for 37.7% of the global gross national product, besides representing more than 50% of the actual global economic growth and 40% of the global population. Footnote 6 Similarly, biomass combustion for combined heat and power production is a carbon sink when combined with CCS.Footnote 7 The more stringent the climate targets become, the more urgent the need for near zero-carbon or negative emissions technologies (NET), a niche that fosters bioenergy with CCS (BECCS).\\n How is the transition away from fossil fuels doing, and how will the low-carbon future unfold?\\n2760 Accesses\\n9 Citations\\n1 Altmetric\\nExplore all metrics\\nGraphic abstract\\nAvoid common mistakes on your manuscript.\\n However, besides economic penalty on the carbon-emitting process, CCS has main drawbacks that increase uncertainty and retards deployments: (i) geological sites for carbon storage are not evenly spread geographically and most often are distant from the carbon emission sources; (ii) public concerns on carbon leakages and consequential effects (e.g., induced seismicity); and (iii) lack of a regulatory framework for post-injection liability. Athos da Silveira Ramos, 149, Centro de Tecnologia, E, Ilha do Fund\\u00e3o, 21941-972, Rio de Janeiro, RJ, Brazil\\nOf\\u00e9lia Q. F. Ara\\u00fajo\\u00a0&\\u00a0Jos\\u00e9 Luiz de Medeiros\\nYou can also search for this author in\\nPubMed\\u00a0Google Scholar\\nYou can also search for this author in\\nPubMed\\u00a0Google Scholar\\nCorresponding author\\nCorrespondence to\\nOf\\u00e9lia Q. F. Ara\\u00fajo.\\n"}], [{"url": "https://unfccc.int/topics/introduction-to-climate-finance", "content": "The UNFCCC website includes a climate finance data portal with helpful explanations, graphics and figures for better understanding the climate finance process and as a gateway to information on activities funded in developing countries to implement climate action. The finance portal comprises three modules, each of which includes information ..."}, {"url": "https://www.worldbank.org/en/news/factsheet/2022/09/30/10-things-you-should-know-about-the-world-bank-group-s-climate-finance", "content": "Did you know\\u2026\\nRELATED\\nWorld Bank - Climate Change\\nClimate Stories: How Countries and Communities Are Shaping a Sustainable Future\\nClimate Explainer Series\\nThis site uses cookies to optimize functionality and give you the best possible experience. 10 Things You Should Know About the World Bank Group\\u2019s Climate Finance\\nPhoto: World Bank\\nFinancing transformative climate action is vital for development and to support the poorest people who are most affected by climate change. With 189 member countries, staff from more than 170 countries, and offices in over 130 locations, the World Bank Group is a unique global partnership: five institutions working for sustainable solutions that reduce poverty and build shared prosperity in developing countries.\\n We provide a wide array of financial products and technical assistance, and we help countries share and apply innovative knowledge and solutions to the challenges they face.\\n Data and research help us understand these challenges and set priorities, share knowledge of what works, and measure progress.\\n"}, {"url": "https://news.un.org/en/story/2021/06/1094762",\n\nEfficiency and Demand\\nCarbon Capture, Utilisation and Storage\\nDecarbonisation Enablers\\nGlobal Energy Transitions Stocktake\\nCritical Minerals\\nRussia\'s War on Ukraine\\nClimate Change\\nGlobal Energy Crisis\\nInvestment\\nSaving Energy\\nEnergy Security\\nNet Zero Emissions\\nEnergy Efficiency\\nData explorers\\nUnderstand and manipulate data with easy to use explorers and trackers\\nData sets\\nFree and paid data sets from across the energy system available for download\\nPolicies database\\nPast, existing or planned government policies and measures\\nChart Library\\nAccess every chart published across all IEA reports and analysis\\nWorld Energy Outlook 2023\\nFlagship report \\u2014 October 2023\\nOil Market Report - December 2023\\nFuel report \\u2014 December 2023\\nEnergy Efficiency 2023\\nFuel report \\u2014 November 2023\\nNet Zero Roadmap: The rapid decarbonisation of the power system is critical for the success of the clean energy transition, since power generation accounts for 40% of energy-related CO2 emissions and electricity is increasingly being used to meet energy demand in key sectors of the economy.\\n The International Energy Agency\\u2019s latest and most comprehensive assessment of clean energy technology progress worldwide shows that a step change in action and ambition is needed across all energy technologies and sectors to keep the goal of net zero emissions by 2050 within reach.\\n Progress on clean energy innovation will be crucial to help develop and deploy the full range of clean energy technologies needed to decarbonise the sectors, in particular those where emissions are the most challenging to reduce, such as aviation, shipping and heavy industry.\\n In transport, stronger policies are needed to encourage shifts to using low-carbon modes of transport, greater energy efficiency measures, and the building out of infrastructure to support zero emission vehicles, as well as the development and uptake of those vehicle in long-distance transport.\\n"}], [{"url": "https://www.iucn.org/our-work/topic/nature-based-solutions-climate", "content": "Enhancing Nature-Based Solutions in Kosovo\\nPublication\\n|\\n2023\\nNature-based Solutions for corporate climate targets\\nNews\\n|\\n09 Nov, 2023\\nReSea Project Launched to Strengthen Coastal Communities in Kenya\\nBlog\\n|\\n01 Nov, 2023\\nTREPA project to plant over 18,000 ha of native species during 2023-2024 tree planting season\\u2026\\nSign up for an IUCN newsletter\\nFeatured bottom second Menus\\nSECRETARIAT\\nCOMMISSIONS\\nTHEMES\\nREGIONS\\nContact\\nHeadquarters\\nRue Mauverney 28\\n1196 Gland\\nSwitzerland\\n+41 22 9990000\\n+41 22 9990002(Fax)\\nFollow Us\\n\\u00a9IUCN, International Union for Conservation of Nature and Natural Resources Nature-based solutions can address climate change in three ways:\\nHeading\\n30%\\nof the global mitigation required by 2030/2050 to achieve the 1.5/2\\u00b0C temperature rise goal agreed to under the Paris Agreement\\nRead more\\nHeading\\n5 GtCO2e\\n5 GtCO2e\\nNature-based Solutions could deliver emission reductions\\nand removals of at least 5 GtCO2e per year by 2030 (of a maximum estimate of 11.7 GtCO2e per year).\\n Learn more\\nHeading\\nUSD 393 Billion\\nwhich can reduce the intensity of climate hazards by 26%\\nRead more\\nIUCN\'s work on NbS for climate\\nIUCN works to advance practical nature-based solutions for both climate mitigation and adaptation, centred on the better conservation, management and restoration of the world\\u2019s ecosystems. IUCN Issues Brief: Ensuring effective Nature-based Solutions\\nAccelerating investment in Nature-based Climate Solutions\\nIUCN supports the acceleration of financing for nature-based solutions for climate change through multiple grant mechanisms, including the Global EbA Fund, the Blue Natural Capital Financing Facility, the Subnational Climate Finance initiative, and the Nature+ Accelerator Fund, which collectively represent 200 million USD in available funding for NbS. Current economic valuation research estimates that an investment of\n\nreducing carbon intensity in a short timeframe (one to two decades). The transition driver is limiting global warming caused by greenhouse gases, majorly emitted by fossil fuels and, to a lesser extent, land-use changes."}, {"url": "https://link.springer.com/article/10.1007/s10098-021-02123-x", "content": "The transition from a fossil-based to a low-carbon economy (based on renewable energies and hydrogen as energy carrier) targets reducing carbon intensity in a short timeframe (one to two decades). The transition driver is limiting global warming caused by greenhouse gases, majorly emitted by fossil fuels and, to a lesser extent, land-use changes."}, {"url": "https://www.anl.gov/sites/www/files/2024-01/Net-Zero-World-Fossil-Transition-Report_FINAL_1-8-2024.pdf", "content": "support to inform community fossil fuel transitions. As a first step, this analysis examines the decision-making processes of fossil fuel transitions in several communities across two countries: the United States and Chile. The goal is a framework that lifts out key decision-making criteria and learnings from communities that have undergone fossil"}], [{"url": "https://www.un.org/en/our-work/support-sustainable-development-and-climate-action", "content": "MDGs \\u2014 Close to 40 per cent of the population of the developing world was ... climate action; life ... a critical role in supporting countries in their efforts to implement the 2030 Agenda by ..."}, {"url": "https://www.worldbank.org/en/topic/climatechange/overview", "content": "Sustainable Development Series\\nThis series offers insights into innovative and state-of-the-art solutions that can guide countries to build more inclusive and sustainable economies that are resilient in the face of pandemics, climate change and other ...\\nIDA and Climate Change\\nIDA helps the poorest nations adapt to climate change by building their resilience to disasters, and promoting sustainable development to minimize their vulnerability.\\n Carbon Pricing Dashboard\\nThis interactive dashboard provides an up-to-date overview of carbon pricing initiatives around the world and allows users to navigate through the visuals and data of the annual State and Trends of Carbon Pricing report ...\\nAdditional Resources\\nRelated\\nContact\\nThis site uses cookies to optimize functionality and give you the best possible experience. Forest Carbon Partnership Facility\\nThe Forest Carbon Partnership Facility is focused on reducing emissions from deforestation and forest degradation, forest carbon stock conservation, the sustainable management of forests, and the enhancement of forest ...\\nBioCarbon Fund Initiative for Sustainable Forest Landscapes\\nThe BioCarbon Fund Initiative for Sustainable Forest Landscapes is focused on reducing emissions from the land sector through smarter land use planning, policies, and practices.\\n The Carbon Pricing Leadership Coalition brings together leaders from across government, the private sector and civil society to share experience working with carbon pricing and to expand the evidence base for the most ...\\nIFC Climate Business\\nIFC invests in the private sector in clean energy, sustainable cities, climate-smart agriculture, energy efficiency, green buildings and green finance.\\n Oct 12, 2023\\nRELATED\\nMULTIMEDIA\\nFinancing the Climate Transition: Building the Green, Inclusive, Resilient Economies of the Future\\nAROUND THE BANK GROUP\\nFind out what the Bank Group\'s branches are doing on climate change.\\n"}, {"url": "https://climatepromise.undp.org/news-and-stories/NDCs-nationally-determined-contributions-climate-change-what-you-need-to-know", "content": "Summary. Nationally Determined Contributions, or NDCs, are countries\' self-defined national climate pledges under the Paris Agreement, detailing what they will do to help meet the global goal to pursue 1.5\\u00b0C, adapt to climate impacts and ensure sufficient finance to support these efforts. NDCs represent short- to medium-term plans and are ..."},' name='retrieve_docs' id='27df1a16-c402-4145-acde-235b1d4933a4' tool_call_id='call_qwwxybfYFcCdeJnPbKwKJwpc'
2024-10-30 17:52:18,785 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-30 18:00:51,261 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors.index.json HTTP/11" 404 0
2024-10-30 18:00:51,411 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors HTTP/11" 302 0
2024-10-31 01:12:19,905 - DEBUG - https://safetensors-convert.hf.space:443 "GET /call/run/84d4fbf99ec64169994988690a623c0c HTTP/11" 200 None
2024-10-31 01:12:21,088 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/commits/main HTTP/11" 200 1920
2024-10-31 01:12:21,341 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/discussions?p=0 HTTP/11" 200 1434
2024-10-31 01:12:21,599 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/commits/refs%2Fpr%2F3 HTTP/11" 200 2885
2024-10-31 01:12:21,692 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors.index.json HTTP/11" 404 0
2024-10-31 01:12:21,949 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors HTTP/11" 302 0
2024-10-31 01:12:22,767 - ERROR - Error in stream_agent_response: 'tool_calls_buffer'
2024-10-31 01:13:06,352 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/commits/refs%2Fpr%2F3 HTTP/11" 200 2885
2024-10-31 01:13:06,354 - DEBUG - Starting new HTTPS connection (1): safetensors-convert.hf.space:443
2024-10-31 01:13:06,926 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-31 01:13:08,549 - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'You are a helpful coding assistant with access to documentation about LangGraph.', 'role': 'system'}, {'content': 'explain this tooL: create_tool Description: Creates a schema for a placeholder tool. Purpose: This function dynamically generates tools based on company names, allowing for flexible tool creation. Example Code: python Copy def create_tool(company: str) -> dict: formatted_company = re.sub(r"[^\\w\\s]", "", company).replace(" ", "_") def company_tool(year: int) -> str: return f"{company} had revenues of $100 in {year}." return StructuredTool.from_function(company_tool, name=formatted_company, description=f"Information about {company}")', 'role': 'user'}], 'model': 'gpt-4o-mini', 'n': 1, 'stream': True, 'temperature': 0.0, 'tools': [{'type': 'function', 'function': {'name': 'retrieve_document_info', 'description': 'Retrieve and format information from documents based on a query.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2024-10-31 01:13:08,550 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2024-10-31 01:13:08,708 - DEBUG - https://safetensors-convert.hf.space:443 "POST /call/run HTTP/11" 200 47
2024-10-31 01:13:08,711 - DEBUG - Starting new HTTPS connection (1): safetensors-convert.hf.space:443
2024-10-31 01:13:08,766 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f590ac1cb20>
2024-10-31 01:13:08,766 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x7f592c0d7640> server_hostname='api.openai.com' timeout=None
2024-10-31 01:13:08,917 - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x7f590ac1c5b0>
2024-10-31 01:13:08,918 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-31 01:13:08,919 - DEBUG - send_request_headers.complete
2024-10-31 01:13:08,919 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-31 01:13:08,920 - DEBUG - send_request_body.complete
2024-10-31 01:13:08,921 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-31 01:13:09,436 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 31 Oct 2024 05:13:09 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-6mqwgmpqixetpcxbmd3dnosa'), (b'openai-processing-ms', b'258'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'1999827'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'5ms'), (b'x-request-id', b'req_6ee3ee84ae883b55854c01a0794dc28e'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Bt656poS_4l5haBhZDnFPV6iBkwC7ocJq.ZkbMtf1bE-1730351589-1.0.1.1-pgwe61dxviqNTCIdb72rVlmRpim0vJn7aM1x8kUjjmpoi77n2uV4_eMoHGBRLfmZXpQiTLjs6.tmLcPaPnQbKw; path=/; expires=Thu, 31-Oct-24 05:43:09 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=llSRaiKEZB66ztdYgUxUT2UrZjW8Q6Lr8qxHilTxsbc-1730351589233-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8db127765d8ea52d-MIA'), (b'alt-svc', b'h3=":443"; ma=86400')])
2024-10-31 01:13:09,437 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-10-31 01:13:09,442 - DEBUG - HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2024-10-31 01:13:09,442 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-31 01:13:09,441 - DEBUG - https://safetensors-convert.hf.space:443 "GET /call/run/b3f7dc37822445cca44789e0b2d30f25 HTTP/11" 200 None
2024-10-31 01:13:09,493 - DEBUG - receive_response_body.complete
2024-10-31 01:13:09,494 - DEBUG - response_closed.started
2024-10-31 01:13:09,494 - DEBUG - response_closed.complete
2024-10-31 01:13:09,499 - DEBUG - Raw Message from LangChain: content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_hc9r7mCAwQ5YbUXbPRnsDfXW', 'function': {'arguments': '{"query":"create_tool"}', 'name': 'retrieve_document_info'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1'} id='run-d713e226-9576-425d-b02a-e68041719ac1-0' tool_calls=[{'name': 'retrieve_document_info', 'args': {'query': 'create_tool'}, 'id': 'call_hc9r7mCAwQ5YbUXbPRnsDfXW', 'type': 'tool_call'}]
2024-10-31 01:13:09,499 - INFO - Structured Response: {'node': 'agent', 'message': {'content': '', 'type': 'AIMessage', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_hc9r7mCAwQ5YbUXbPRnsDfXW', 'function': {'arguments': '{"query":"create_tool"}', 'name': 'retrieve_document_info'}, 'type': 'function'}], 'type': 'AIMessage'}}}
2024-10-31 01:13:09,864 - DEBUG - Starting component PersistentLocalHnswSegment
2024-10-31 01:13:09,898 - DEBUG - Raw Message from LangChain: content='\nResult 1:\nOrigin: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/extraction/retries.ipynb\nPurpose: To bind a validation function to the LLM\'s output and retry the LLM with a fallback strategy if the validation fails.\nKeywords: validation, retry, fallback, tool calling, LLM\nCode Snippet:\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f"{var}: ")\n\n\n_set_env("OPENAI_API_KEY")\n\nimport operator\nimport uuid\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Type,\n    Union,\n)\n\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.messages import (\n    AIMessage,\n    AnyMessage,\n    BaseMessage,\n    HumanMessage,\n    ToolCall,\n)\nfrom langchain_core.prompt_values import PromptValue\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableLambda,\n)\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ValidationNode\n\n\ndef _default_aggregator(messages: Sequence[AnyMessage]) -> AIMessage:\n    for m in messages[::-1]:\n        if m.type == "ai":\n            return m\n    raise ValueError("No AI message found in the sequence.")\n\n\nclass RetryStrategy(TypedDict, total=False):\n    """The retry strategy for a tool call."""\n\n    max_attempts: int\n    """The maximum number of attempts to make."""\n    fallback: Optional[\n        Union[\n            Runnable[Sequence[AnyMessage], AIMessage],\n            Runnable[Sequence[AnyMessage], BaseMessage],\n            Callable[[Sequence[AnyMessage]], AIMessage],\n        ]\n    ]\n    """The function to use once validation fails."""\n    aggregate_messages: Optional[Callable[[Sequence[AnyMessage]], AIMessage]]\n\n\ndef _bind_validator_with_retries(\n    llm: Union[\n        Runnable[Sequence[AnyMessage], AIMessage],\n        Runnable[Sequence[BaseMessage], BaseMessage],\n    ],\n    *,\n    validator: ValidationNode,\n    retry_strategy: RetryStrategy,\n    tool_choice: Optional[str] = None,\n) -> Runnable[Union[List[AnyMessage], PromptValue], AIMessage]:\n    """Binds a tool validators + retry logic to create a runnable validation graph.\n\n    LLMs that support tool calling can generate structured JSON. However, they may not always\n    perfectly follow your requested schema, especially if the schema is nested or has complex\n    validation rules. This method allows you to bind a validation function to the LLM\'s output,\n    so that any time the LLM generates a message, the validation function is run on it. If\n    the validation fails, the method will retry the LLM with a fallback strategy, the simplest\n    being just to add a message to the output with the validation errors and a request to fix them.\n\n    The resulting runnable expects a list of messages as input and returns a single AI message.\n    By default, the LLM can optionally NOT invoke tools, making this easier to incorporate into\n    your existing chat bot. You can specify a tool_choice to force the validator to be run on\n    the outputs.\n\n    Args:\n        llm (Runnable): The llm that will generate the initial messages (and optionally fallba)\n        validator (ValidationNode): The validation logic.\n        retry_strategy (RetryStrategy): The retry strategy to use.\n            Possible keys:\n            - max_attempts: The maximum number of attempts to make.\n            - fallback: The LLM or function to use in case of validation failure.\n            - aggregate_messages: A function to aggregate the messages over multiple turns.\n                Defaults to fetching the last AI message.\n        tool_choice: If provided, always run the validator on the tool output.\n\n    Returns:\n        Runnable: A runnable that can be invoked with a list of messages and returns a single AI message.\n    """\n\n    def add_or_overwrite_messages(left: list, right: Union[list, dict]) -> list:\n        """Append messages. If the update is a \'finalized\' output, replace the whole list."""\n        if isinstance(right, dict) and "finalize" in right:\n            finalized = right["finalize"]\n            if not isinstance(finalized, list):\n                finalized = [finalized]\n            for m in finalized:\n                if m.id is None:\n                    m.id = str(uuid.uuid4())\n            return finalized\n        res = add_messages(left, right)\n        if not isinstance(res, list):\n            return [res]\n        return res\n\n    class State(TypedDict):\n        messages: Annotated[list, add_or_overwrite_messages]\n        attempt_number: Annotated[int, operator.add]\n        initial_num_messages: int\n        input_format: Literal["list", "dict"]\n\n    builder = StateGraph(State)\n\n    def dedict(x: State) -> list:\n        """Get the messages from the state."""\n        return x["messages"]\n\n    model = dedict | llm | (lambda msg: {"messages": [msg], "attempt_number": 1})\n    fbrunnable = retry_strategy.get("fallback")\n    if fbrunnable is None:\n        fb_runnable = llm\n    elif isinstance(fbrunnable, Runnable):\n        fb_runnable = fbrunnable  # type: ignore\n    else:\n        fb_runnable = RunnableLambda(fbrunnable)\n    fallback = (\n        dedict | fb_runnable | (lambda msg: {"messages": [msg], "attempt_number": 1})\n    )\n\n    def count_messages(state: State) -> dict:\n        return {"initial_num_messages": len(state.get("messages", []))}\n\n    builder.add_node("count_messages", count_messages)\n    builder.add_node("llm", model)\n    builder.add_node("fallback", fallback)\n\n    # To support patch-based retries, we need to be able to\n    # aggregate the messages over multiple turns.\n    # The next sequence selects only the relevant messages\n    # and then applies the validator\n    select_messages = retry_strategy.get("aggregate_messages") or _default_aggregator\n\n    def select_generated_messages(state: State) -> list:\n        """Select only the messages generated within this loop."""\n        selected = state["messages"][state["initial_num_messages"] :]\n        return [select_messages(selected)]\n\n    def endict_validator_output(x: Sequence[AnyMessage]) -> dict:\n        if tool_choice and not x:\n            return {\n                "messages": [\n                    HumanMessage(\n                        content=f"ValidationError: please respond with a valid tool call [tool_choice={tool_choice}].",\n                        additional_kwargs={"is_error": True},\n                    )\n                ]\n            }\n        return {"messages": x}\n\n    validator_runnable = select_generated_messages | validator | endict_validator_output\n    builder.add_node("validator", validator_runnable)\n\n    class Finalizer:\n        """Pick the final message to return from the retry loop."""\n\n        def __init__(self, aggregator: Optional[Callable[[list], AIMessage]] = None):\n            self._aggregator = aggregator or _default_aggregator\n\n        def __call__(self, state: State) -> dict:\n            """Return just the AI message."""\n            initial_num_messages = state["initial_num_messages"]\n            generated_messages = state["messages"][initial_num_messages:]\n            return {\n                "messages": {\n                    "finalize": self._aggregator(generated_messages),\n                }\n            }\n\n    # We only want to emit the final message\n    builder.add_node("finalizer", Finalizer(retry_strategy.get("aggregate_messages")))\n\n    # Define the connectivity\n    builder.add_edge(START, "count_messages")\n    builder.add_edge("count_messages", "llm")\n\n    def route_validator(state: State):\n        if state["messages"][-1].tool_calls or tool_choice is not None:\n            return "validator"\n        return END\n\n    builder.add_conditional_edges("llm", route_validator, ["validator", END])\n    builder.add_edge("fallback", "validator")\n    max_attempts = retry_strategy.get("max_attempts", 3)\n\n    def route_validation(state: State):\n        if state["attempt_number"] > max_attempts:\n            raise ValueError(\n                f"Could not extract a valid value in {max_attempts} attempts."\n            )\n        for m in state["messages"][::-1]:\n            if m.type == "ai":\n                break\n            if m.additional_kwargs.get("is_error"):\n                return "fallback"\n        return "finalizer"\n\n    builder.add_conditional_edges(\n        "validator", route_validation, ["finalizer", "fallback"]\n    )\n\n    builder.add_edge("finalizer", END)\n\n    # These functions let the step be used in a MessageGraph\n    # or a StateGraph with \'messages\' as the key.\n    def encode(x: Union[Sequence[AnyMessage], PromptValue]) -> dict:\n        """Ensure the input is the correct format."""\n        if isinstance(x, PromptValue):\n            return {"messages": x.to_messages(), "input_format": "list"}\n        if isinstance(x, list):\n            return {"messages": x, "input_format": "list"}\n        raise ValueError(f"Unexpected input type: {type(x)}")\n\n    def decode(x: State) -> AIMessage:\n        """Ensure the output is in the expected format."""\n        return x["messages"][-1]\n\n    return (\n        encode | builder.compile().with_config(run_name="ValidationGraph") | decode\n    ).with_config(run_name="ValidateWithRetries")\n--------------------------------------------------------------------------------\n\nResult 2:\nOrigin: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/streaming-events-from-within-tools.ipynb\nPurpose: Processing Jupyter notebook code\nKeywords: Jupyter notebook, code processing, clean code, metadata generation, JSON output\nCode Snippet:\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f"{var}: ")\n\n\n_set_env("OPENAI_API_KEY")\n\nfrom langchain_core.callbacks import Callbacks\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\n\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_openai import ChatOpenAI\n\n\n@tool\nasync def get_items(\n    place: str,\n    callbacks: Callbacks,\n) -> str:\n    return await llm.ainvoke(\n        [\n            {\n                "role": "user",\n                "content": f"Can you tell me what kind of items i might find in the following place: \'{place}\'. "\n                "List at least 3 such items separating them by a comma. And include a brief description of each item..",\n            }\n        ],\n        {"callbacks": callbacks},\n    )\n\n\nllm = ChatOpenAI(model_name="gpt-4o")\ntools = [get_items]\nagent = create_react_agent(llm, tools=tools)\n\nfinal_message = ""\nasync for msg, metadata in agent.astream(\n    {"messages": [("human", "what items are on the shelf?")]}, stream_mode="messages"\n):\n    if (\n        msg.content\n        and not isinstance(msg, HumanMessage)\n        and metadata["langgraph_node"] == "tools"\n        and not msg.name\n    ):\n        print(msg.content, end="|", flush=True)\n    if msg.content and metadata["langgraph_node"] == "agent":\n        final_message += msg.content\n\n\nasync for event in agent.astream_events(\n    {"messages": [{"role": "user", "content": "what\'s in the bedroom."}]}, version="v2"\n):\n    if (\n        event["event"] == "on_chat_model_stream"\n        and event["metadata"].get("langgraph_node") == "tools"\n    ):\n        print(event["data"]["chunk"].content, end="|", flush=True)\n--------------------------------------------------------------------------------\n\nResult 3:\nOrigin: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/pass-run-time-values-to-tools.ipynb\nPurpose: Processing Jupyter notebook code\nKeywords: Jupyter notebook, code processing, clean code, metadata generation, JSON output\nCode Snippet:\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f"{var}: ")\n\n\n_set_env("OPENAI_API_KEY")\n\n\nfrom typing import List\n\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f"{var}: ")\n\n\n_set_env("OPENAI_API_KEY")\n\n\nfrom typing import List, Tuple\nfrom typing_extensions import Annotated\n\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import InjectedState\n\n\n@tool\ndef get_context(question: str, state: Annotated[dict, InjectedState]):\n    return "\\n\\n".join(doc for doc in state["docs"])\n\n\nfrom langgraph.prebuilt import ToolNode, create_react_agent\nfrom langgraph.checkpoint.memory import MemorySaver\n\n\nmodel = ChatOpenAI(model="gpt-4o", temperature=0)\ntools = [get_context]\n\ntool_node = ToolNode(tools)\n\ncheckpointer = MemorySaver()\ngraph = create_react_agent(model, tools, state_schema=State, checkpointer=checkpointer)\n\ndocs = [\n    "FooBar company just raised 1 Billion dollars!",\n    "FooBar company was founded in 2019",\n]\n\ninputs = {\n    "messages": [{"type": "user", "content": "what\'s the latest news about FooBar"}],\n    "docs": docs,\n}\nconfig = {"configurable": {"thread_id": "1"}}\nfor chunk in graph.stream(inputs, config, stream_mode="values"):\n    chunk["messages"][-1].pretty_print()\n\n\nfrom langgraph.store.memory import InMemoryStore\n\n\ndoc_store = InMemoryStore()\n\n\nnamespace = ("documents", "1")\ndoc_store.put(\n    namespace, "doc_0", {"doc": "FooBar company just raised 1 Billion dollars!"}\n)\nnamespace = ("documents", "2")\ndoc_store.put(namespace, "doc_1", {"doc": "FooBar company was founded in 2019"})\n\n\nfrom langgraph.store.base import BaseStore\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt import InjectedStore\n\n\n@tool\ndef get_context(\n    question: str,\n    config: RunnableConfig,\n    store: Annotated[BaseStore, InjectedStore()],\n) -> Tuple[str, List[Document]]:\n    user_id = config.get("configurable", {}).get("user_id")\n    docs = [item.value["doc"] for item in store.search(("documents", user_id))]\n    return "\\n\\n".join(doc for doc in docs)\n\n\ntools = [get_context]\n\ntool_node = ToolNode(tools)\n\ncheckpointer = MemorySaver()\ngraph = create_react_agent(model, tools, checkpointer=checkpointer, store=doc_store)\n\nmessages = [{"type": "user", "content": "what\'s the latest news about FooBar"}]\nconfig = {"configurable": {"thread_id": "1", "user_id": "1"}}\nfor chunk in graph.stream({"messages": messages}, config, stream_mode="values"):\n    chunk["messages"][-1].pretty_print()\n\nmessages = [{"type": "user", "content": "what\'s the latest news about FooBar"}]\nconfig = {"configurable": {"thread_id": "2", "user_id": "2"}}\nfor chunk in graph.stream({"messages": messages}, config, stream_mode="values"):\n    chunk["messages"][-1].pretty_print()\n--------------------------------------------------------------------------------\n\nResult 4:\nOrigin: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/react-agent-from-scratch.ipynb\nPurpose: Processing Jupyter notebook code\nKeywords: unknown\nCode Snippet:\n{\n    "code": "import getpass\\nimport os\\n\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\\"{var}: \\")\\n\\n\\n_set_env(\\"OPENAI_API_KEY\\")\\n\\n\\nfrom typing import (\\n    Annotated,\\n    Sequence,\\n    TypedDict,\\n)\\nfrom langchain_core.messages import BaseMessage\\nfrom langgraph.graph.message import add_messages\\n\\n\\n\\nclass AgentState(TypedDict):\\n    \\"""The state of the agent.\\""\\"\\n\\n    # add_messages is a reducer\\n    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\\n    messages: Annotated[Sequence[BaseMessage], add_messages]\\n\\n\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.tools import tool\\n\\nmodel = ChatOpenAI(model=\\"gpt-4o-mini\\")\\n\\n\\n@tool\\ndef get_weather(location: str):\\n    \\"""Call to get the weather from a specific location.\\""\\"\\n    # This is a placeholder for the actual implementation\\n    # Don\'t let the LLM know this though üòä\\n    if any([city in location.lower() for city in [\\"sf\\", \\"san francisco\\"]]):\\n        return \\"It\'s sunny in San Francisco, but you better look out if you\'re a Gemini üòà.\\"\\n    else:\\n        return f\\"I am not sure what the weather is in {location}\\"\\n\\n\\ntools = [get_weather]\\n\\nmodel = model.bind_tools(tools)\\n\\n\\nimport json\\nfrom langchain_core.messages import ToolMessage, SystemMessage\\nfrom langchain_core.runnables import RunnableConfig\\n\\ntools_by_name = {tool.name: tool for tool in tools}\\n\\n\\n# Define our tool node\\ndef tool_node(state: AgentState):\\n    outputs = []\\n    for tool_call in state[\\"messages\\"][-1].tool_calls:\\n        tool_result = tools_by_name[tool_call[\\"name\\"]].invoke(tool_call[\\"args\\"])\\n        outputs.append(\\n            ToolMessage(\\n                content=json.dumps(tool_result),\\n                name=tool_call[\\"name\\"],\\n                tool_call_id=tool_call[\\"id\\"],\\n            )\\n        )\\n    return {\\"messages\\": outputs}\\n\\n\\n# Define the node that calls the model\\ndef call_model(\\n    state: AgentState,\\n    config: RunnableConfig,\\n):\\n    # this is similar to customizing the create_react_agent with state_modifier, but is a lot more flexible\\n    system_prompt = SystemMessage(\\n        \\"You are a helpful AI assistant, please respond to the users query to the best of your ability!\\"\\n    )\\n    response = model.invoke([system_prompt] + state[\\"messages\\"], config)\\n    # We return a list, because this will get added to the existing list\\n    return {\\"messages\\": [response]}\\n\\n\\n# Define the conditional edge that determines whether to continue or not\\ndef should_continue(state: AgentState):\\n    messages = state[\\"messages\\"]\\n    last_message = messages[-1]\\n    # If there is no function call, then we finish\\n    if not last_message.tool_calls:\\n        return \\"end\\"\\n    # Otherwise if there is, we continue\\n    else:\\n        return \\"continue\\"\\n\\n\\nfrom langgraph.graph import StateGraph, END\\n\\n# Define a new graph\\nworkflow = StateGraph(AgentState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\\"agent\\", call_model)\\nworkflow.add_node(\\"tools\\", tool_node)\\n\\n# Set the entrypoint as `agent`\\n# This means that this node is the first one called\\nworkflow.set_entry_point(\\"agent\\")\\n\\n# We now add a conditional edge\\nworkflow.add_conditional_edges(\\n    # First, we define the start node. We use `agent`.\\n    # This means these are the edges taken after the `agent` node is called.\\n    \\"agent\\",\\n    # Next, we pass in the function that will determine which node is called next.\\n    should_continue,\\n    # Finally we pass in a mapping.\\n    # The keys are strings, and the values are other nodes.\\n    # END is a special node marking that the graph should finish.\\n    # What will happen is we will call `should_continue`, and then the output of that\\n    # will be matched against the keys in this mapping.\\n    # Based on which one it matches, that node will then be called.\\n    {\\n        # If `tools`, then we call the tool node.\\n        \\"continue\\": \\"tools\\",\\n        # Otherwise we finish.\\n        \\"end\\": END,\\n    },\\n)\\n\\n# We now add a normal edge from `tools` to `agent`.\\n# This means that after `tools` is called, `agent` node is called next.\\nworkflow.add_edge(\\"tools\\", \\"agent\\")\\n\\n# Now we can compile and visualize our graph\\ngraph = workflow.compile()\\n\\n\\nfrom IPython.display import Image, display\\n\\ntry:\\n    display(Image(graph.get_graph().draw_mermaid_png()))\\nexcept Exception:\\n    # This requires some extra dependencies and is optional\\n    pass\\n\\n\\n# Helper function for formatting the stream nicely\\ndef print_stream(stream):\\n    for s in stream:\\n        message = s[\\"messages\\"][-1]\\n        if isinstance(message, tuple):\\n            print(message)\\n        else:\\n            message.pretty_print()\\n\\n\\ninputs = {\\"messages\\": [(\\"user\\", \\"what is the weather in sf\\")]}\\nprint_stream(graph.stream(inputs, stream_mode=\\"values\\"))",\n    "metadata": {\n        "purpose": "Processing Jupyter notebook code",\n        "usage": "To clean the code, generate metadata, and provide output in JSON format",\n        "keywords": [\n            "Jupyter notebook",\n            "code processing",\n            "clean code",\n            "metadata generation",\n            "JSON output"\n        ],\n        "functions": [\n            {\n                "name": "_set_env",\n                "description": "Sets environment variable"\n            },\n            {\n                "name": "get_weather",\n                "description": "Call to get the weather from a specific location"\n            },\n            {\n                "name": "tool_node",\n                "description": "Node that handles tool calls"\n            },\n            {\n                "name": "call_model",\n                "description": "Node that calls the model"\n            },\n            {\n                "name": "should_continue",\n                "description": "Conditional edge that determines whether to continue or not"\n            },\n            {\n                "name": "print_stream",\n                "description": "Helper function for formatting the stream"\n            }\n        ],\n        "concepts": [\n            {\n                "name": "AgentState",\n                "description": "The state of the agent"\n            },\n            {\n                "name": "add_messages",\n                "description": "Reducer function for adding messages"\n            },\n            {\n                "name": "ChatOpenAI",\n                "description": "Class for interacting with OpenAI chat models"\n            },\n            {\n                "name": "tool",\n                "description": "Decorator for defining a tool function"\n            },\n            {\n                "name": "ToolMessage",\n                "description": "Class representing a message from a tool"\n            },\n            {\n                "name": "SystemMessage",\n                "description": "Class representing a system message"\n            },\n            {\n                "name": "RunnableConfig",\n                "description": "Configuration for running a runnable"\n            },\n            {\n                "name": "StateGraph",\n                "description": "Class representing a state graph"\n            },\n            {\n                "name": "END",\n                "description": "Special node marking the end of the graph"\n            }\n        ]\n    }\n}\n--------------------------------------------------------------------------------\n\nResult 5:\nOrigin: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/how-tos/node-retries.ipynb\nPurpose: Processing Jupyter notebook code\nKeywords: Jupyter notebook, code processing, clean code, metadata generation, JSON output\nCode Snippet:\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f"{var}: ")\n\n\n_set_env("ANTHROPIC_API_KEY")\n\n\nimport operator\nimport sqlite3\nfrom typing import Annotated, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.messages import BaseMessage\n\nfrom langgraph.graph import END, StateGraph, START\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_core.messages import AIMessage\n\ndb = SQLDatabase.from_uri("sqlite:///:memory:")\n\nmodel = ChatAnthropic(model_name="claude-2.1")\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n\n\ndef query_database(state):\n    query_result = db.run("SELECT * FROM Artist LIMIT 10;")\n    return {"messages": [AIMessage(content=query_result)]}\n\n\ndef call_model(state):\n    response = model.invoke(state["messages"])\n    return {"messages": [response]}\n\n\n# Define a new graph\nbuilder = StateGraph(AgentState)\nbuilder.add_node(\n    "query_database",\n    query_database,\n    retry=RetryPolicy(retry_on=sqlite3.OperationalError),\n)\nbuilder.add_node("model", call_model, retry=RetryPolicy(max_attempts=5))\nbuilder.add_edge(START, "model")\nbuilder.add_edge("model", "query_database")\nbuilder.add_edge("query_database", END)\n\ngraph = builder.compile()\n--------------------------------------------------------------------------------' name='retrieve_document_info' id='a610a41c-8c7a-4947-af45-b1f2e682f9ff' tool_call_id='call_hc9r7mCAwQ5YbUXbPRnsDfXW'
2024-10-31 01:13:10,551 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/commits/main HTTP/11" 200 1920
2024-10-31 01:13:10,553 - DEBUG - https://us.i.posthog.com:443 "POST /batch/ HTTP/11" 200 15
2024-10-31 01:13:10,648 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/discussions?p=0 HTTP/11" 200 1434
2024-10-31 01:13:11,063 - DEBUG - https://huggingface.co:443 "GET /api/models/microsoft/graphcodebert-base/commits/refs%2Fpr%2F3 HTTP/11" 200 2885
2024-10-31 01:13:11,151 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors.index.json HTTP/11" 404 0
2024-10-31 01:13:11,716 - DEBUG - https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/refs%2Fpr%2F3/model.safetensors HTTP/11" 302 0
