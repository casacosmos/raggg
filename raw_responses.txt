{"node": "agent", "message": {"content": "", "type": "AIMessage", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_7xojI0S7CVpBUX7U9tNESCJS", "function": {"arguments": "{\"query\":\"LangGraph\"}", "name": "retrieve_docs"}, "type": "function"}]}}}
{"node": "action", "message": {"content": "Langgraph adaptive rag local\n\n%%capture --no-stderr\n%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4\n\nLocal RAG agent with LLaMA3\u00b6\nWe'll combine ideas from paper RAG papers into a RAG agent:\n\nRouting:  Adaptive RAG (paper). Route questions to different retrieval approaches\nFallback: Corrective RAG (paper). Fallback to web search if docs are not relevant to query\nSelf-correction: Self-RAG (paper). Fix answers w/ hallucinations or don\u2019t address question\n\nLocal models\u00b6\nEmbedding\u00b6\nGPT4All Embeddings:\npip install langchain-nomic\n\nLLM\u00b6\nUse Ollama and llama3.2:\nollama pull llama3.2:3b-instruct-fp16 \n\n### LLM\nfrom langchain_ollama import ChatOllama\n\nlocal_llm = \"llama3.2:3b-instruct-fp16\"\nllm = ChatOllama(model=local_llm, temperature=0)\nllm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n\nSearch\u00b6\nFor search, we use Tavily, which is a search engine optimized for LLMs and RAG.\nimport os\nimport getpass\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"TAVILY_API_KEY\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nTracing\u00b6\nOptionally, use LangSmith for tracing. \n_set_env(\"LANGCHAIN_API_KEY\")\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\"\n\nVectorstore\u00b6\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import SKLearnVectorStore\nfrom langchain_nomic.embeddings import NomicEmbeddings\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=200\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = SKLearnVectorStore.from_documents(\n    documents=doc_splits,\n    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(k=3)\n\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | SKLearnVectorStore | NomicEmbeddings\n\nComponents\u00b6\n### Router\nimport json\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# Prompt\nrouter_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n\nUse the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n\nReturn JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n\n# Test router\ntest_web_search = llm_json_mode.invoke(\n    [SystemMessage(content=router_instructions)]\n    + [\n        HumanMessage(\n            content=\"Who is favored to win the NFC Championship game in the 2024 season?\"\n        )\n    ]\n)\ntest_web_search_2 = llm_json_mode.invoke(\n    [SystemMessage(content=router_instructions)]\n    + [HumanMessage(content=\"What are the models released today for llama3.2?\")]\n)\ntest_vector_store = llm_json_mode.invoke(\n    [SystemMessage(content=router_instructions)]\n    + [HumanMessage(content=\"What are the types of agent memory?\")]\n)\nprint(\n    json.loads(test_web_search.content),\n    json.loads(test_web_search_2.content),\n    json.loads(test_vector_store.content),\n)\n\n{'datasource': 'websearch'} {'datasource': 'websearch'} {'datasource': 'vectorstore'}\n\nAPI Reference:\nHumanMessage | SystemMessage\n\nJS/TS\n\nJS/TS\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk\u00b6\nClasses\u00b6\n\nAssistantsClient\nClient\nCronsClient\nRunsClient\nStoreClient\nThreadsClient\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / AssistantsClient\nClass: AssistantsClient\u00b6\nExtends\u00b6\n\nBaseClient\n\nConstructors\u00b6\nnew AssistantsClient()\u00b6\n\nnew AssistantsClient(config?): AssistantsClient\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nAssistantsClient\nInherited from\u00b6\nBaseClient.constructor\nDefined in\u00b6\nclient.ts:52\nMethods\u00b6\ncreate()\u00b6\n\ncreate(payload): Promise\\<Assistant>\n\nCreate a new assistant.\nParameters\u00b6\n\u2022 payload\nPayload for creating an assistant.\n\u2022 payload.assistantId?: string\n\u2022 payload.config?: Config\n\u2022 payload.graphId: string\n\u2022 payload.ifExists?: OnConflictBehavior\n\u2022 payload.metadata?: Metadata\n\u2022 payload.name?: string\nReturns\u00b6\nPromise\\<Assistant>\nThe created assistant.\nDefined in\u00b6\nclient.ts:295\n\ndelete()\u00b6\n\ndelete(assistantId): Promise\\<void>\n\nDelete an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:347\n\nget()\u00b6\n\nget(assistantId): Promise\\<Assistant>\n\nGet an assistant by ID.\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant.\nReturns\u00b6\nPromise\\<Assistant>\nAssistant\nDefined in\u00b6\nclient.ts:237\n\ngetGraph()\u00b6\n\ngetGraph(assistantId, options?): Promise\\<AssistantGraph>\n\nGet the JSON representation of the graph assigned to a runnable\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant.\n\u2022 options?\n\u2022 options.xray?: number | boolean\nWhether to include subgraphs in the serialized graph representation. If an integer value is provided, only subgraphs with a depth less than or equal to the value will be included.\nReturns\u00b6\nPromise\\<AssistantGraph>\nSerialized graph\nDefined in\u00b6\nclient.ts:247\n\ngetSchemas()\u00b6\n\ngetSchemas(assistantId): Promise\\<GraphSchema>\n\nGet the state and config schema of the graph assigned to a runnable\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant.\nReturns\u00b6\nPromise\\<GraphSchema>\nGraph schema\nDefined in\u00b6\nclient.ts:261\n\ngetSubgraphs()\u00b6\n\ngetSubgraphs(assistantId, options?): Promise\\<Subgraphs>\n\nGet the schemas of an assistant by ID.\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant to get the schema of.\n\u2022 options?\nAdditional options for getting subgraphs, such as namespace or recursion extraction.\n\u2022 options.namespace?: string\n\u2022 options.recurse?: boolean\nReturns\u00b6\nPromise\\<Subgraphs>\nThe subgraphs of the assistant.\nDefined in\u00b6\nclient.ts:272\n\ngetVersions()\u00b6\n\ngetVersions(assistantId, payload?): Promise\\<AssistantVersion[]>\n\nList all versions of an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\n\u2022 payload?\n\u2022 payload.limit?: number\n\u2022 payload.metadata?: Metadata\n\u2022 payload.offset?: number\nReturns\u00b6\nPromise\\<AssistantVersion[]>\nList of assistant versions.\nDefined in\u00b6\nclient.ts:381\n\nsearch()\u00b6\n\nsearch(query?): Promise\\<Assistant[]>\n\nList assistants.\nParameters\u00b6\n\u2022 query?\nQuery options.\n\u2022 query.graphId?: string\n\u2022 query.limit?: number\n\u2022 query.metadata?: Metadata\n\u2022 query.offset?: number\nReturns\u00b6\nPromise\\<Assistant[]>\nList of assistants.\nDefined in\u00b6\nclient.ts:358\n\nsetLatest()\u00b6\n\nsetLatest(assistantId, version): Promise\\<Assistant>\n\nChange the version of an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\n\u2022 version: number\nThe version to change to.\nReturns\u00b6\nPromise\\<Assistant>\nThe updated assistant.\nDefined in\u00b6\nclient.ts:409\n\nupdate()\u00b6\n\nupdate(assistantId, payload): Promise\\<Assistant>\n\nUpdate an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\n\u2022 payload\nPayload for updating the assistant.\n\u2022 payload.config?: Config\n\u2022 payload.graphId?: string\n\u2022 payload.metadata?: Metadata\n\u2022 payload.name?: string\nReturns\u00b6\nPromise\\<Assistant>\nThe updated assistant.\nDefined in\u00b6\nclient.ts:322\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / Client\nClass: Client\u00b6\nConstructors\u00b6\nnew Client()\u00b6\n\nnew Client(config?): Client\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nClient\nDefined in\u00b6\nclient.ts:1233\nProperties\u00b6\nassistants\u00b6\n\nAdaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader\n\nnew Client(config?): Client\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nClient\nDefined in\u00b6\nclient.ts:1233\nProperties\u00b6\nassistants\u00b6\n\nassistants: AssistantsClient\n\nThe client for interacting with assistants.\nDefined in\u00b6\nclient.ts:1211\n\ncrons\u00b6\n\ncrons: CronsClient\n\nThe client for interacting with cron runs.\nDefined in\u00b6\nclient.ts:1226\n\nruns\u00b6\n\nruns: RunsClient\n\nThe client for interacting with runs.\nDefined in\u00b6\nclient.ts:1221\n\nstore\u00b6\n\nstore: StoreClient\n\nThe client for interacting with the KV store.\nDefined in\u00b6\nclient.ts:1231\n\nthreads\u00b6\n\nthreads: ThreadsClient\n\nThe client for interacting with threads.\nDefined in\u00b6\nclient.ts:1216\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / CronsClient\nClass: CronsClient\u00b6\nExtends\u00b6\n\nBaseClient\n\nConstructors\u00b6\nnew CronsClient()\u00b6\n\nnew CronsClient(config?): CronsClient\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nCronsClient\nInherited from\u00b6\nBaseClient.constructor\nDefined in\u00b6\nclient.ts:52\nMethods\u00b6\ncreate()\u00b6\n\ncreate(assistantId, payload?): Promise\\<Run>\n\nParameters\u00b6\n\u2022 assistantId: string\nAssistant ID to use for this cron job.\n\u2022 payload?: CronsCreatePayload\nPayload for creating a cron job.\nReturns\u00b6\nPromise\\<Run>\nDefined in\u00b6\nclient.ts:176\n\ncreateForThread()\u00b6\n\ncreateForThread(threadId, assistantId, payload?): Promise\\<Run>\n\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 assistantId: string\nAssistant ID to use for this cron job.\n\u2022 payload?: CronsCreatePayload\nPayload for creating a cron job.\nReturns\u00b6\nPromise\\<Run>\nThe created background run.\nDefined in\u00b6\nclient.ts:148\n\ndelete()\u00b6\n\ndelete(cronId): Promise\\<void>\n\nParameters\u00b6\n\u2022 cronId: string\nCron ID of Cron job to delete.\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:201\n\nsearch()\u00b6\n\nsearch(query?): Promise\\<Cron[]>\n\nParameters\u00b6\n\u2022 query?\nQuery options.\n\u2022 query.assistantId?: string\n\u2022 query.limit?: number\n\u2022 query.offset?: number\n\u2022 query.threadId?: string\nReturns\u00b6\nPromise\\<Cron[]>\nList of crons.\nDefined in\u00b6\nclient.ts:212\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / RunsClient\nClass: RunsClient\u00b6\nExtends\u00b6\n\nBaseClient\n\nConstructors\u00b6\nnew RunsClient()\u00b6\n\nnew RunsClient(config?): RunsClient\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nRunsClient\nInherited from\u00b6\nBaseClient.constructor\nDefined in\u00b6\nclient.ts:52\nMethods\u00b6\ncancel()\u00b6\n\ncancel(threadId, runId, wait): Promise\\<void>\n\nCancel a run.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\n\u2022 wait: boolean = false\nWhether to block when canceling\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:928\n\ncreate()\u00b6\n\ncreate(threadId, assistantId, payload?): Promise\\<Run>\n\nCreate a run.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 assistantId: string\nAssistant ID to use for this run.\n\u2022 payload?: RunsCreatePayload\nPayload for creating a run.\nReturns\u00b6\nPromise\\<Run>\nThe created run.\nDefined in\u00b6\nclient.ts:765\n\ncreateBatch()\u00b6\n\ncreateBatch(payloads): Promise\\<Run[]>\n\nCreate a batch of stateless background runs.\nParameters\u00b6\n\u2022 payloads: RunsCreatePayload & object[]\nAn array of payloads for creating runs.\nReturns\u00b6\nPromise\\<Run[]>\nAn array of created runs.\nDefined in\u00b6\nclient.ts:796\n\ndelete()\u00b6\n\ndelete(threadId, runId): Promise\\<void>\n\nDelete a run.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:1032\n\nget()\u00b6\n\nget(threadId, runId): Promise\\<Run>\n\nGet a run by ID.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\nReturns\u00b6\nPromise\\<Run>\nThe run.\nDefined in\u00b6\nclient.ts:916\n\njoin()\u00b6\n\njoin(threadId, runId, options?): Promise\\<void>\n\nBlock until a run is done.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\n\u2022 options?\n\u2022 options.signal?: AbortSignal\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:948\n\njoinStream()\u00b6\n\njoinStream(threadId, runId, signal?): AsyncGenerator\\<object, any, unknown>\n\nAgentic RAG\n\nAgentic RAG\u00b6\nRetrieval Agents are useful when we want to make decisions about whether to retrieve from an index.\nTo implement a retrieval agent, we simply need to give an LLM access to a retriever tool.\nWe can incorporate this into LangGraph.\nSetup\u00b6\nFirst, let's download the required packages and set our API keys:\n%%capture --no-stderr\n%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters\n\nimport getpass\nimport os\n\ndef _set_env(key: str):\n    if key not in os.environ:\n        os.environ[key] = getpass.getpass(f\"{key}:\")\n\n_set_env(\"OPENAI_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nRetriever\u00b6\nFirst, we index 3 blog posts.\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nWebBaseLoader | Chroma | OpenAIEmbeddings | RecursiveCharacterTextSplitter\n\nThen we create a retriever tool.\nfrom langchain.tools.retriever import create_retriever_tool\n\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_blog_posts\",\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n)\n\ntools = [retriever_tool]\n\nAPI Reference:\ncreate_retriever_tool\n\nAgent State\u00b6\nWe will define a graph.\nA state object that it passes around to each node.\nOur state will be a list of messages.\nEach node in our graph will append to it.\nfrom typing import Annotated, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.messages import BaseMessage\n\nfrom langgraph.graph.message import add_messages\n\nclass AgentState(TypedDict):\n    # The add_messages function defines how an update should be processed\n    # Default is to replace. add_messages says \"append\"\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\nAPI Reference:\nBaseMessage | add_messages\n\nNodes and Edges\u00b6\nWe can lay out an agentic RAG graph like this:\n\nThe state is a set of messages\nEach node will update (append to) state\nConditional edges decide which node to visit next\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain import hub\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\nfrom langgraph.prebuilt import tools_condition\n\n### Edges\n\ndef grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.", "type": "ToolMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "LangGraph is a framework designed to facilitate the development, deployment, and management of large language model (LLM) applications, particularly those that integrate retrieval-augmented generation (RAG) techniques. It provides tools and infrastructure to create complex LLM applications that can route queries, fallback to web search when necessary, self-correct answers, and manage interactions with various data sources and models.\n\nKey components and features of LangGraph include:\n\n1. **Adaptive RAG**: This approach combines query analysis with active or self-corrective RAG techniques. It allows for routing queries to different retrieval approaches based on the nature of the query, such as no retrieval, single-shot RAG, or iterative RAG. This adaptability ensures that the most appropriate method is used for each query, optimizing both the relevance and quality of the responses.\n\n2. **Local Models and Tools**: LangGraph supports the use of local models for embedding and language model inference, including GPT4All Embeddings and LLaMA models. It also integrates with search engines optimized for LLMs and RAG, such as Tavily, and provides tools for document loading, text splitting, and vector storage.\n\n3. **Development Tools**: LangGraph offers integration with development tools like LangSmith for tracing and debugging LLM applications. This helps developers quickly identify and resolve issues, improving the performance and reliability of their applications.\n\n4. **API and SDK**: LangGraph provides a comprehensive API and SDK, including classes and methods for managing assistants, crons, runs, stores, and threads. This allows developers to programmatically interact with LangGraph components, creating sophisticated LLM applications that can handle complex workflows and data interactions.\n\n5. **Retrieval Agents**: LangGraph supports the implementation of retrieval agents, which make decisions about whether to retrieve from an index or not. This is particularly useful for applications that need to dynamically determine the best source of information based on the query context.\n\n6. **Agentic RAG**: This concept involves the use of retrieval agents within a RAG framework to decide on the best course of action for answering queries. It includes setting up retriever tools, defining agent states, and creating nodes and edges for decision-making processes within the application.\n\nLangGraph is designed to be a comprehensive solution for developers looking to leverage the power of LLMs and RAG in their applications, providing the necessary tools and infrastructure to build, deploy, and manage complex LLM applications efficiently.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "Hello! How can I assist you today?", "type": "AIMessage", "additional_kwargs": {}}}
{"error": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}"}
{"node": "agent", "message": {"content": "", "type": "AIMessage", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_tcN0vYL5Kyd7thhEgEkjW2Zd", "function": {"arguments": "{\"query\":\"what is LangGraph\"}", "name": "retrieve_docs"}, "type": "function"}]}}}
{"node": "action", "message": {"content": "================================[1m Human Message [0m=================================\n\nwhat is the weather in sf\n==================================[1m Ai Message [0m==================================\nTool Calls:\n  get_weather (call_azW0cQ4XjWWj0IAkWAxq9nLB)\n Call ID: call_azW0cQ4XjWWj0IAkWAxq9nLB\n  Args:\n    location: San Francisco\n=================================[1m Tool Message [0m=================================\nName: get_weather\n\n\"It's sunny in San Francisco, but you better look out if you're a Gemini \\ud83d\\ude08.\"\n==================================[1m Ai Message [0m==================================\n\nThe weather in San Francisco is sunny! However, it seems there's a playful warning for Geminis. Enjoy the sunshine!\n\nPerfect! The graph correctly calls the get_weather tool and responds to the user after receiving the information from the tool.\n\n  \n\n  Comments\n\n================================[1m Human Message [0m=================================\n\nwhat's the latest news about FooBar\n==================================[1m Ai Message [0m==================================\nTool Calls:\n  get_context (call_UkqfR7z2cLJQjhatUpDeEa5H)\n Call ID: call_UkqfR7z2cLJQjhatUpDeEa5H\n  Args:\n    question: latest news about FooBar\n=================================[1m Tool Message [0m=================================\nName: get_context\n\nFooBar company just raised 1 Billion dollars!\n\nFooBar company was founded in 2019\n==================================[1m Ai Message [0m==================================\n\nThe latest news about FooBar is that the company has just raised 1 billion dollars.\n\nPass shared memory (store) to the graph\u00b6\nYou might also want to give tools access to memory that is shared across multiple conversations or users. We can do it by passing LangGraph Store to the tools using a different annotation -- InjectedStore.\nLet's modify our example to save the documents in an in-memory store and retrieve them using get_context tool. We'll also make the documents accessible based on a user ID, so that some documents are only visible to certain users. The tool will then use the user_id provided in the config to retrieve a correct set of documents.\n\nNote\n\n        Support for Store API and InjectedStore used in this notebook was added in LangGraph v0.2.34.\n        \n\nInjectedStore annotation requires langchain-core >= 0.3.8\n\nfrom langgraph.store.memory import InMemoryStore\n\ndoc_store = InMemoryStore()\n\nnamespace = (\"documents\", \"1\")  # user ID\ndoc_store.put(\n    namespace, \"doc_0\", {\"doc\": \"FooBar company just raised 1 Billion dollars!\"}\n)\nnamespace = (\"documents\", \"2\")  # user ID\ndoc_store.put(namespace, \"doc_1\", {\"doc\": \"FooBar company was founded in 2019\"})\n\nDefine the tools\u00b6\nfrom langgraph.store.base import BaseStore\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt import InjectedStore\n\n@tool\ndef get_context(\n    question: str,\n    config: RunnableConfig,\n    store: Annotated[BaseStore, InjectedStore()],\n) -> Tuple[str, List[Document]]:\n    \"\"\"Get relevant context for answering the question.\"\"\"\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n    docs = [item.value[\"doc\"] for item in store.search((\"documents\", user_id))]\n    return \"\\n\\n\".join(doc for doc in docs)\n\nAPI Reference:\nRunnableConfig\n\nWe can also verify that the tool-calling model will ignore store arg of get_context tool:\nget_context.tool_call_schema.schema()\n\n{'description': 'Get relevant context for answering the question.',\n 'properties': {'question': {'title': 'Question', 'type': 'string'}},\n 'required': ['question'],\n 'title': 'get_context',\n 'type': 'object'}\n\nDefine the graph\u00b6\nLet's update our ReAct agent:\ntools = [get_context]\n\n# ToolNode will automatically take care of injecting Store into tools\ntool_node = ToolNode(tools)\n\ncheckpointer = MemorySaver()\n# NOTE: we need to pass our store to `create_react_agent` to make sure our graph is aware of it\ngraph = create_react_agent(model, tools, checkpointer=checkpointer, store=doc_store)\n\nUse it!\u00b6\nLet's try running our graph with a \"user_id\" in the config.\nmessages = [{\"type\": \"user\", \"content\": \"what's the latest news about FooBar\"}]\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\nfor chunk in graph.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nwhat's the latest news about FooBar\n==================================[1m Ai Message [0m==================================\nTool Calls:\n  get_context (call_ocyHBpGgF3LPFOgRKURBfkGG)\n Call ID: call_ocyHBpGgF3LPFOgRKURBfkGG\n  Args:\n    question: latest news about FooBar\n=================================[1m Tool Message [0m=================================\nName: get_context\n\n- Take a day trip to Lucerne and go see the iconic Chapel Bridge, Lion Monument, and do a lake cruise\n- Visit the Rhine Falls near Schaffhausen - one of the largest waterfalls in Europe\n- Take a guided walking tour through Basel's old town to see the red sandstone buildings and historical sites\n- Do a day trip into the Swiss Alps, potentially taking a cogwheel train up into the mountains\n- Tour the medieval Ch\u00e2teau de Bottmingen just outside of Basel\n- Take a day trip across the border to explore the Alsace wine region of France\n- Visit the Fondation Beyeler museum that houses an impressive modern art collection\n\nLet me know if you'd like me to book any specific tours/excursions from those options, or if you prefer to just have the rental car flexibility to explore Basel and surroundings at your own pace. I'm happy to make excursion bookings or you can play that portion by ear once there. Just let me know what you'd prefer!\n================================[1m Human Message [0m=================================\n\nAre they available while I'm there?\n==================================[1m Ai Message [0m==================================\n\n[{'text': 'Good point, let me check availability for some of those recommended Basel/Swiss excursions and activities during your stay from May 8th to 15th:', 'type': 'text'}, {'id': 'toolu_01GjChRNrPMhtrrFquKeGsoa', 'input': {'keywords': 'lucerne day trip, swiss alps tour, basel walking tour, alsace wine tour', 'location': 'basel'}, 'name': 'search_trip_recommendations', 'type': 'tool_use'}]\nTool Calls:\n  search_trip_recommendations (toolu_01GjChRNrPMhtrrFquKeGsoa)\n Call ID: toolu_01GjChRNrPMhtrrFquKeGsoa\n  Args:\n    keywords: lucerne day trip, swiss alps tour, basel walking tour, alsace wine tour\n    location: basel\n=================================[1m Tool Message [0m=================================\nName: search_trip_recommendations\n\n[]\n==================================[1m Ai Message [0m==================================\n\nUnfortunately it does not look like my searches are returning any bookable tours or excursions in the Basel area for those date ranges. The database seems to be lacking comprehensive options.\n\nAs an alternative, let me suggest just keeping your schedule flexible during your stay. With your rental car, you can easily do self-guided day trips to places like:\n\n- Lucerne (1.5 hour drive)\n- Bern (1 hour drive) \n- Zurich (1 hour drive)\n- Rhine Falls (45 min drive)\n- Alsace, France (1 hour drive)\n\nAnd in Basel itself, you can explore at your own pace hitting top sights like:\n\n- Basel Munster cathedral \n- Old Town\n- Basel Paper Mill Museum\n- Rhine river promenades\n\nThere are also several highly-rated free walking tour companies that operate daily in Basel you could join.\n\nRather than pre-booking rigid excursions, having the rental car will give you maximum flexibility to pick and choose what you want to do day-to-day based on your interests and the weather.\n\nLet me know if you'd still like me to continue searching for pre-bookable tours, or if you're okay winging it and using the rental car to explore Basel and do day trips during your week there.\n================================[1m Human Message [0m=================================\n\ninteresting - i like the museums, what options are there? \n==================================[1m Ai Message [0m==================================\n\n[]\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's strange, my search isn't returning any recommendations for tours, day trips or excursions in Switzerland. Let me do one more general search for activities:\", 'type': 'text'}, {'id': 'toolu_0174DPmee4i1r91hxs1UJCSF', 'input': {'keywords': 'activities switzerland'}, 'name': 'search_trip_recommendations', 'type': 'tool_use'}]\nTool Calls:\n  search_trip_recommendations (toolu_0174DPmee4i1r91hxs1UJCSF)\n Call ID: toolu_0174DPmee4i1r91hxs1UJCSF\n  Args:\n    keywords: activities switzerland\n=================================[1m Tool Message [0m=================================\nName: search_trip_recommendations\n\n[]\n==================================[1m Ai Message [0m==================================\n\nI'm really struggling to find any recommended activities, tours or excursions to book for your stay in the Basel area. It seems the database may be lacking robust options for that region. \n\nInstead, here are a few potential ideas I could recommend based on some quick research:\n\n- Take a day trip to Lucerne and go see the iconic Chapel Bridge and Lion Monument\n- Visit the Swiss Vapeur Parc, an amusement park focused on trains and transportation\n- Go for a hike up Gempenplateau for scenic views overlooking Basel\n- Take a food tour to sample the local Swiss cuisine like r\u00f6sti and fondue\n- Do a wine tasting day trip out to the vineyards near Alsace, France\n\nLet me know if any of those appeal to you or if you'd like me to find some other moderate activity recommendations for your Basel stay. I can also hold off on booking excursions for now if you prefer to play that portion by ear once there. Just let me know your preference!\n================================[1m Human Message [0m=================================\n\nNow for a car, what are my options?\n==================================[1m Ai Message [0m==================================\n\n[{'text': 'No problem, let me provide some additional rental car options for you during your stay in Basel from May 8th to May 15th.', 'type': 'text'}, {'id': 'toolu_012CmfeoLyidUpZ1AP22AaU4', 'input': {'end_date': '2024-05-15', 'location': 'Basel', 'start_date': '2024-05-08'}, 'name': 'search_car_rentals', 'type': 'tool_use'}]\nTool Calls:\n  search_car_rentals (toolu_012CmfeoLyidUpZ1AP22AaU4)\n Call ID: toolu_012CmfeoLyidUpZ1AP22AaU4\n  Args:\n    end_date: 2024-05-15\n    location: Basel\n    start_date: 2024-05-08\n=================================[1m Tool Message [0m=================================\nName: search_car_rentals\n\n[{\"id\": 1, \"name\": \"Europcar\", \"location\": \"Basel\", \"price_tier\": \"Economy\", \"start_date\": \"2024-04-14\", \"end_date\": \"2024-04-11\", \"booked\": 1}, {\"id\": 2, \"name\": \"Avis\", \"location\": \"Basel\", \"price_tier\": \"Luxury\", \"start_date\": \"2024-04-10\", \"end_date\": \"2024-04-20\", \"booked\": 0}, {\"id\": 7, \"name\": \"Enterprise\", \"location\": \"Basel\", \"price_tier\": \"Premium\", \"start_date\": \"2024-04-22\", \"end_date\": \"2024-04-20\", \"booked\": 0}, {\"id\": 9, \"name\": \"Thrifty\", \"location\": \"Basel\", \"price_tier\": \"Midsize\", \"start_date\": \"2024-04-17\", \"end_date\": \"2024-04-26\", \"booked\": 0}]\n==================================[1m Ai Message [0m==================================\n\nI see a few options still available for those dates:\n\nAvis - Luxury Tier\nEnterprise - Premium Tier \nThrifty - Midsize Tier\n\nI had previously booked the Economy tier Europcar rental already.\n\nWould you like me to book one of the other available tiers? The Midsize from Thrifty may be a decent moderate option between Economy and Premium/Luxury. Let me know which rental car option you'd prefer and I can get that booked.\n\nOr if you need to adjust the pickup/drop-off dates or location, I can search again with those new parameters as well. Just let me know your rental car preferences!\n================================[1m Human Message [0m=================================\n\ninteresting - i like the outdoors. OK great let's book that then.\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"Got it, since you mentioned enjoying outdoor activities, let's focus on booking an excursion that showcases the beautiful natural scenery around Basel.\\n\\nBased on my initial recommendations, I think going hiking or biking through the vineyards and villages in the Basel countryside would be a great outdoor option during your stay next week.\", 'type': 'text'}, {'id': 'toolu_01R1xCvXzTx1HvCvtjEr5fVX', 'input': {'location': 'Basel', 'request': 'Please book a guided hiking or biking tour through the vineyards and rural villages outside of Basel for sometime during the week of May 4-11, 2024. Preference for an outdoor nature/scenery focused excursion.'}, 'name': 'BookExcursion', 'type': 'tool_use'}]\nTool Calls:\n  BookExcursion (toolu_01R1xCvXzTx1HvCvtjEr5fVX)\n Call ID: toolu_01R1xCvXzTx1HvCvtjEr5fVX\n  Args:\n    location: Basel\n    request: Please book a guided hiking or biking tour through the vineyards and rural villages outside of Basel for sometime during the week of May 4-11, 2024. Preference for an outdoor nature/scenery focused excursion.\nCurrently in:  book_excursion\n=================================[1m Tool Message [0m=================================\n\nThe assistant is now the Trip Recommendation Assistant. Reflect on the above conversation between the host assistant and the user. The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are Trip Recommendation Assistant, and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool. If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control. Do not mention who you are - just act as the proxy for the assistant.\nCurrently in:  book_excursion\n==================================[1m Ai Message [0m==================================\n\n[{'text': 'Sounds like the perfect excursion for you! Let me look into available guided hiking and biking tours showcasing the scenic Basel countryside during your stay next week.', 'type': 'text'}, {'id': 'toolu_01QU6mk1NhNeRWTtaiChkhuN', 'input': {'keywords': 'hiking, biking, nature, scenery, vineyards, villages', 'location': 'Basel', 'start_date': '2024-05-04', 'end_date': '2024-05-11'}, 'name': 'search_trip_recommendations', 'type': 'tool_use'}]\nTool Calls:\n  search_trip_recommendations (toolu_01QU6mk1NhNeRWTtaiChkhuN)\n Call ID: toolu_01QU6mk1NhNeRWTtaiChkhuN\n  Args:\n    keywords: hiking, biking, nature, scenery, vineyards, villages\n    location: Basel\n    start_date: 2024-05-04\n    end_date: 2024-05-11\nCurrently in:  book_excursion\n=================================[1m Tool Message [0m=================================\nName: search_trip_recommendations\n\n[]\nCurrently in:  book_excursion\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"Hmm it doesn't look like my initial search pulled up any vineyard hiking or biking tours for those dates. Let me try expanding the search:\", 'type': 'text'}, {'id': 'toolu_019K14VeBY9LANAZgrqTNNso', 'input': {'keywords': 'outdoors, nature, countryside, bike tour, hike', 'location': 'Basel', 'start_date': '2024-05-01', 'end_date': '2024-05-15'}, 'name': 'search_trip_recommendations', 'type': 'tool_use'}]\nTool Calls:\n  search_trip_recommendations (toolu_019K14VeBY9LANAZgrqTNNso)\n Call ID: toolu_019K14VeBY9LANAZgrqTNNso\n  Args:\n    keywords: outdoors, nature, countryside, bike tour, hike\n    location: Basel\n    start_date: 2024-05-01\n    end_date: 2024-05-15\nCurrently in:  book_excursion\n=================================[1m Tool Message [0m=================================\nName: search_trip_recommendations\n\n[]\nCurrently in:  book_excursion\n==================================[1m Ai Message [0m==================================", "type": "ToolMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "It seems there was a misunderstanding in retrieving the information you requested about LangGraph. Let me correct that and provide you with the details about LangGraph.\n\nLangGraph is a framework designed to facilitate the integration and interaction of various tools and services within a conversational AI context. It allows for the creation of complex, multi-step workflows that can leverage different AI models, APIs, and custom logic to process and respond to user inputs. This framework is particularly useful for building advanced AI assistants capable of performing a wide range of tasks, from answering questions to executing actions based on user requests.\n\nKey features of LangGraph include:\n\n- **Tool Integration:** LangGraph can integrate with a variety of tools, including language models, databases, and external APIs, enabling a rich set of functionalities.\n- **Memory and Context Management:** It supports memory and context management, allowing the AI to maintain state across interactions. This is crucial for handling multi-turn conversations and remembering user preferences or previous interactions.\n- **Customizable Workflows:** Developers can define custom workflows, specifying how different tools and models interact with each other. This allows for the creation of tailored experiences that can adapt to specific user needs or domains.\n- **Scalability:** LangGraph is designed to be scalable, supporting the development of complex AI systems that can handle a large number of users and interactions.\n\nIn essence, LangGraph serves as a backbone for developing sophisticated conversational AI applications, providing the infrastructure needed to combine multiple AI capabilities into cohesive and intelligent systems.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "Hello! How can I assist you today?", "type": "AIMessage", "additional_kwargs": {}}}
{"error": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}"}
{"node": "agent", "message": {"content": "", "type": "AIMessage", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_InPLNc0Ra0sREQDmcFFo7YJH", "function": {"arguments": "{\"query\":\"LangGraph\"}", "name": "retrieve_docs"}, "type": "function"}]}}}
{"node": "action", "message": {"content": "Langgraph adaptive rag local\n\n%%capture --no-stderr\n%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4\n\nLocal RAG agent with LLaMA3\u00b6\nWe'll combine ideas from paper RAG papers into a RAG agent:\n\nRouting:  Adaptive RAG (paper). Route questions to different retrieval approaches\nFallback: Corrective RAG (paper). Fallback to web search if docs are not relevant to query\nSelf-correction: Self-RAG (paper). Fix answers w/ hallucinations or don\u2019t address question\n\nLocal models\u00b6\nEmbedding\u00b6\nGPT4All Embeddings:\npip install langchain-nomic\n\nLLM\u00b6\nUse Ollama and llama3.2:\nollama pull llama3.2:3b-instruct-fp16 \n\n### LLM\nfrom langchain_ollama import ChatOllama\n\nlocal_llm = \"llama3.2:3b-instruct-fp16\"\nllm = ChatOllama(model=local_llm, temperature=0)\nllm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n\nSearch\u00b6\nFor search, we use Tavily, which is a search engine optimized for LLMs and RAG.\nimport os\nimport getpass\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"TAVILY_API_KEY\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nTracing\u00b6\nOptionally, use LangSmith for tracing. \n_set_env(\"LANGCHAIN_API_KEY\")\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\"\n\nVectorstore\u00b6\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import SKLearnVectorStore\nfrom langchain_nomic.embeddings import NomicEmbeddings\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=200\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = SKLearnVectorStore.from_documents(\n    documents=doc_splits,\n    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(k=3)\n\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | SKLearnVectorStore | NomicEmbeddings\n\nComponents\u00b6\n### Router\nimport json\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# Prompt\nrouter_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n\nUse the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n\nReturn JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n\n# Test router\ntest_web_search = llm_json_mode.invoke(\n    [SystemMessage(content=router_instructions)]\n    + [\n        HumanMessage(\n            content=\"Who is favored to win the NFC Championship game in the 2024 season?\"\n        )\n    ]\n)\ntest_web_search_2 = llm_json_mode.invoke(\n    [SystemMessage(content=router_instructions)]\n    + [HumanMessage(content=\"What are the models released today for llama3.2?\")]\n)\ntest_vector_store = llm_json_mode.invoke(\n    [SystemMessage(content=router_instructions)]\n    + [HumanMessage(content=\"What are the types of agent memory?\")]\n)\nprint(\n    json.loads(test_web_search.content),\n    json.loads(test_web_search_2.content),\n    json.loads(test_vector_store.content),\n)\n\n{'datasource': 'websearch'} {'datasource': 'websearch'} {'datasource': 'vectorstore'}\n\nAPI Reference:\nHumanMessage | SystemMessage\n\nJS/TS\n\nJS/TS\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk\u00b6\nClasses\u00b6\n\nAssistantsClient\nClient\nCronsClient\nRunsClient\nStoreClient\nThreadsClient\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / AssistantsClient\nClass: AssistantsClient\u00b6\nExtends\u00b6\n\nBaseClient\n\nConstructors\u00b6\nnew AssistantsClient()\u00b6\n\nnew AssistantsClient(config?): AssistantsClient\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nAssistantsClient\nInherited from\u00b6\nBaseClient.constructor\nDefined in\u00b6\nclient.ts:52\nMethods\u00b6\ncreate()\u00b6\n\ncreate(payload): Promise\\<Assistant>\n\nCreate a new assistant.\nParameters\u00b6\n\u2022 payload\nPayload for creating an assistant.\n\u2022 payload.assistantId?: string\n\u2022 payload.config?: Config\n\u2022 payload.graphId: string\n\u2022 payload.ifExists?: OnConflictBehavior\n\u2022 payload.metadata?: Metadata\n\u2022 payload.name?: string\nReturns\u00b6\nPromise\\<Assistant>\nThe created assistant.\nDefined in\u00b6\nclient.ts:295\n\ndelete()\u00b6\n\ndelete(assistantId): Promise\\<void>\n\nDelete an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:347\n\nget()\u00b6\n\nget(assistantId): Promise\\<Assistant>\n\nGet an assistant by ID.\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant.\nReturns\u00b6\nPromise\\<Assistant>\nAssistant\nDefined in\u00b6\nclient.ts:237\n\ngetGraph()\u00b6\n\ngetGraph(assistantId, options?): Promise\\<AssistantGraph>\n\nGet the JSON representation of the graph assigned to a runnable\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant.\n\u2022 options?\n\u2022 options.xray?: number | boolean\nWhether to include subgraphs in the serialized graph representation. If an integer value is provided, only subgraphs with a depth less than or equal to the value will be included.\nReturns\u00b6\nPromise\\<AssistantGraph>\nSerialized graph\nDefined in\u00b6\nclient.ts:247\n\ngetSchemas()\u00b6\n\ngetSchemas(assistantId): Promise\\<GraphSchema>\n\nGet the state and config schema of the graph assigned to a runnable\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant.\nReturns\u00b6\nPromise\\<GraphSchema>\nGraph schema\nDefined in\u00b6\nclient.ts:261\n\ngetSubgraphs()\u00b6\n\ngetSubgraphs(assistantId, options?): Promise\\<Subgraphs>\n\nGet the schemas of an assistant by ID.\nParameters\u00b6\n\u2022 assistantId: string\nThe ID of the assistant to get the schema of.\n\u2022 options?\nAdditional options for getting subgraphs, such as namespace or recursion extraction.\n\u2022 options.namespace?: string\n\u2022 options.recurse?: boolean\nReturns\u00b6\nPromise\\<Subgraphs>\nThe subgraphs of the assistant.\nDefined in\u00b6\nclient.ts:272\n\ngetVersions()\u00b6\n\ngetVersions(assistantId, payload?): Promise\\<AssistantVersion[]>\n\nList all versions of an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\n\u2022 payload?\n\u2022 payload.limit?: number\n\u2022 payload.metadata?: Metadata\n\u2022 payload.offset?: number\nReturns\u00b6\nPromise\\<AssistantVersion[]>\nList of assistant versions.\nDefined in\u00b6\nclient.ts:381\n\nsearch()\u00b6\n\nsearch(query?): Promise\\<Assistant[]>\n\nList assistants.\nParameters\u00b6\n\u2022 query?\nQuery options.\n\u2022 query.graphId?: string\n\u2022 query.limit?: number\n\u2022 query.metadata?: Metadata\n\u2022 query.offset?: number\nReturns\u00b6\nPromise\\<Assistant[]>\nList of assistants.\nDefined in\u00b6\nclient.ts:358\n\nsetLatest()\u00b6\n\nsetLatest(assistantId, version): Promise\\<Assistant>\n\nChange the version of an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\n\u2022 version: number\nThe version to change to.\nReturns\u00b6\nPromise\\<Assistant>\nThe updated assistant.\nDefined in\u00b6\nclient.ts:409\n\nupdate()\u00b6\n\nupdate(assistantId, payload): Promise\\<Assistant>\n\nUpdate an assistant.\nParameters\u00b6\n\u2022 assistantId: string\nID of the assistant.\n\u2022 payload\nPayload for updating the assistant.\n\u2022 payload.config?: Config\n\u2022 payload.graphId?: string\n\u2022 payload.metadata?: Metadata\n\u2022 payload.name?: string\nReturns\u00b6\nPromise\\<Assistant>\nThe updated assistant.\nDefined in\u00b6\nclient.ts:322\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / Client\nClass: Client\u00b6\nConstructors\u00b6\nnew Client()\u00b6\n\nnew Client(config?): Client\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nClient\nDefined in\u00b6\nclient.ts:1233\nProperties\u00b6\nassistants\u00b6\n\nAdaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader\n\nnew Client(config?): Client\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nClient\nDefined in\u00b6\nclient.ts:1233\nProperties\u00b6\nassistants\u00b6\n\nassistants: AssistantsClient\n\nThe client for interacting with assistants.\nDefined in\u00b6\nclient.ts:1211\n\ncrons\u00b6\n\ncrons: CronsClient\n\nThe client for interacting with cron runs.\nDefined in\u00b6\nclient.ts:1226\n\nruns\u00b6\n\nruns: RunsClient\n\nThe client for interacting with runs.\nDefined in\u00b6\nclient.ts:1221\n\nstore\u00b6\n\nstore: StoreClient\n\nThe client for interacting with the KV store.\nDefined in\u00b6\nclient.ts:1231\n\nthreads\u00b6\n\nthreads: ThreadsClient\n\nThe client for interacting with threads.\nDefined in\u00b6\nclient.ts:1216\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / CronsClient\nClass: CronsClient\u00b6\nExtends\u00b6\n\nBaseClient\n\nConstructors\u00b6\nnew CronsClient()\u00b6\n\nnew CronsClient(config?): CronsClient\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nCronsClient\nInherited from\u00b6\nBaseClient.constructor\nDefined in\u00b6\nclient.ts:52\nMethods\u00b6\ncreate()\u00b6\n\ncreate(assistantId, payload?): Promise\\<Run>\n\nParameters\u00b6\n\u2022 assistantId: string\nAssistant ID to use for this cron job.\n\u2022 payload?: CronsCreatePayload\nPayload for creating a cron job.\nReturns\u00b6\nPromise\\<Run>\nDefined in\u00b6\nclient.ts:176\n\ncreateForThread()\u00b6\n\ncreateForThread(threadId, assistantId, payload?): Promise\\<Run>\n\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 assistantId: string\nAssistant ID to use for this cron job.\n\u2022 payload?: CronsCreatePayload\nPayload for creating a cron job.\nReturns\u00b6\nPromise\\<Run>\nThe created background run.\nDefined in\u00b6\nclient.ts:148\n\ndelete()\u00b6\n\ndelete(cronId): Promise\\<void>\n\nParameters\u00b6\n\u2022 cronId: string\nCron ID of Cron job to delete.\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:201\n\nsearch()\u00b6\n\nsearch(query?): Promise\\<Cron[]>\n\nParameters\u00b6\n\u2022 query?\nQuery options.\n\u2022 query.assistantId?: string\n\u2022 query.limit?: number\n\u2022 query.offset?: number\n\u2022 query.threadId?: string\nReturns\u00b6\nPromise\\<Cron[]>\nList of crons.\nDefined in\u00b6\nclient.ts:212\n\n@langchain/langgraph-sdk \u2022 Docs\n\n@langchain/langgraph-sdk / RunsClient\nClass: RunsClient\u00b6\nExtends\u00b6\n\nBaseClient\n\nConstructors\u00b6\nnew RunsClient()\u00b6\n\nnew RunsClient(config?): RunsClient\n\nParameters\u00b6\n\u2022 config?: ClientConfig\nReturns\u00b6\nRunsClient\nInherited from\u00b6\nBaseClient.constructor\nDefined in\u00b6\nclient.ts:52\nMethods\u00b6\ncancel()\u00b6\n\ncancel(threadId, runId, wait): Promise\\<void>\n\nCancel a run.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\n\u2022 wait: boolean = false\nWhether to block when canceling\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:928\n\ncreate()\u00b6\n\ncreate(threadId, assistantId, payload?): Promise\\<Run>\n\nCreate a run.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 assistantId: string\nAssistant ID to use for this run.\n\u2022 payload?: RunsCreatePayload\nPayload for creating a run.\nReturns\u00b6\nPromise\\<Run>\nThe created run.\nDefined in\u00b6\nclient.ts:765\n\ncreateBatch()\u00b6\n\ncreateBatch(payloads): Promise\\<Run[]>\n\nCreate a batch of stateless background runs.\nParameters\u00b6\n\u2022 payloads: RunsCreatePayload & object[]\nAn array of payloads for creating runs.\nReturns\u00b6\nPromise\\<Run[]>\nAn array of created runs.\nDefined in\u00b6\nclient.ts:796\n\ndelete()\u00b6\n\ndelete(threadId, runId): Promise\\<void>\n\nDelete a run.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:1032\n\nget()\u00b6\n\nget(threadId, runId): Promise\\<Run>\n\nGet a run by ID.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\nReturns\u00b6\nPromise\\<Run>\nThe run.\nDefined in\u00b6\nclient.ts:916\n\njoin()\u00b6\n\njoin(threadId, runId, options?): Promise\\<void>\n\nBlock until a run is done.\nParameters\u00b6\n\u2022 threadId: string\nThe ID of the thread.\n\u2022 runId: string\nThe ID of the run.\n\u2022 options?\n\u2022 options.signal?: AbortSignal\nReturns\u00b6\nPromise\\<void>\nDefined in\u00b6\nclient.ts:948\n\njoinStream()\u00b6\n\njoinStream(threadId, runId, signal?): AsyncGenerator\\<object, any, unknown>\n\nAgentic RAG\n\nAgentic RAG\u00b6\nRetrieval Agents are useful when we want to make decisions about whether to retrieve from an index.\nTo implement a retrieval agent, we simply need to give an LLM access to a retriever tool.\nWe can incorporate this into LangGraph.\nSetup\u00b6\nFirst, let's download the required packages and set our API keys:\n%%capture --no-stderr\n%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters\n\nimport getpass\nimport os\n\ndef _set_env(key: str):\n    if key not in os.environ:\n        os.environ[key] = getpass.getpass(f\"{key}:\")\n\n_set_env(\"OPENAI_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nRetriever\u00b6\nFirst, we index 3 blog posts.\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nWebBaseLoader | Chroma | OpenAIEmbeddings | RecursiveCharacterTextSplitter\n\nThen we create a retriever tool.\nfrom langchain.tools.retriever import create_retriever_tool\n\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_blog_posts\",\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n)\n\ntools = [retriever_tool]\n\nAPI Reference:\ncreate_retriever_tool\n\nAgent State\u00b6\nWe will define a graph.\nA state object that it passes around to each node.\nOur state will be a list of messages.\nEach node in our graph will append to it.\nfrom typing import Annotated, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.messages import BaseMessage\n\nfrom langgraph.graph.message import add_messages\n\nclass AgentState(TypedDict):\n    # The add_messages function defines how an update should be processed\n    # Default is to replace. add_messages says \"append\"\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\nAPI Reference:\nBaseMessage | add_messages\n\nNodes and Edges\u00b6\nWe can lay out an agentic RAG graph like this:\n\nThe state is a set of messages\nEach node will update (append to) state\nConditional edges decide which node to visit next\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\n\nfrom langchain import hub\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\nfrom langgraph.prebuilt import tools_condition\n\n### Edges\n\ndef grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.", "type": "ToolMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "LangGraph is a framework designed to facilitate the development, deployment, and management of large language model (LLM) applications, particularly those that involve complex interactions, data retrieval, and decision-making processes. It provides a structured way to build applications that leverage the capabilities of LLMs, such as GPT-3, LLaMA, and others, by organizing the logic into graphs that define how information flows and decisions are made within the application.\n\nKey components and concepts of LangGraph include:\n\n1. **Adaptive RAG (Retrieval-Augmented Generation):** This approach combines different retrieval strategies to route questions to the most appropriate sources of information. It can dynamically choose between using a vectorstore (a database optimized for vector search), web search, or self-correction mechanisms based on the relevance of the available documents to the query.\n\n2. **Local Models and Tools:**\n   - **Embedding Models:** For creating vector representations of text, which are essential for searching documents in a vectorstore.\n   - **LLMs:** Leveraging models like Ollama and LLaMA for generating responses or processing queries.\n   - **Search Engines:** Utilizing specialized search engines like Tavily, optimized for integration with LLMs.\n   - **Vectorstores:** Databases like SKLearnVectorStore or Chroma that store and allow for efficient search of vectorized documents.\n\n3. **Development Tools:**\n   - **LangSmith:** A tool for tracing and debugging LangGraph applications, helping developers spot issues and optimize performance.\n   - **Document Loaders and Text Splitters:** Utilities for loading, processing, and preparing documents to be indexed in a vectorstore.\n\n4. **Graph Components:**\n   - **Router:** A component that decides whether a query should be routed to a vectorstore or a web search based on the query's content and the relevance of the indexed documents.\n   - **Retrieval Grader:** A mechanism for assessing the relevance of retrieved documents and deciding the next steps, such as generating a response directly or refining the search.\n   - **Agentic RAG:** A strategy for creating retrieval agents that decide whether and how to retrieve information from an index, enhancing the application's ability to provide relevant and accurate responses.\n\n5. **SDK and API:** LangGraph offers a software development kit (SDK) and API for building and managing applications. This includes classes and methods for creating assistants, managing documents, and interacting with the underlying graph structure.\n\nLangGraph is designed to be flexible and extensible, allowing developers to build sophisticated LLM applications that can handle complex queries, interact with users in a conversational manner, and retrieve and synthesize information from diverse sources.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "It seems like you might have accidentally submitted your request before finishing your question. Could you provide more details or clarify what specific information or assistance you need?", "type": "AIMessage", "additional_kwargs": {}}}
{"error": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}"}
{"node": "agent", "message": {"content": "", "type": "AIMessage", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_ZmVGAYkShHk99vKkJSt115ap", "function": {"arguments": "{\"query\":\"react agent code example\"}", "name": "retrieve_docs"}, "type": "function"}]}}}
{"node": "action", "message": {"content": "Receiving new event of type: debug...\n\nAdaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader\n\nCode generation with RAG and self-correction\n\nCode generation with RAG and self-correction\u00b6\nAlphaCodium presented an approach for code generation that uses control flow.\nMain idea: construct an answer to a coding question iteratively.. \nAlphaCodium iteravely tests and improves an answer on public and AI-generated tests for a particular question. \nWe will implement some of these ideas from scratch using LangGraph:\n\nWe start with a set of documentation specified by a user\nWe use a long context LLM to ingest it and perform RAG to answer a question based upon it\nWe will invoke a tool to produce a structured output\nWe will perform two unit tests (check imports and code execution) prior returning the solution to the user \n\nSetup\u00b6\nFirst, let's install our required packages and set the API keys we will need\n! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"ANTHROPIC_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nDocs\u00b6\nLoad LangChain Expression Language (LCEL) docs as an example.\nfrom bs4 import BeautifulSoup as Soup\nfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n\n# LCEL docs\nurl = \"https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel\"\nloader = RecursiveUrlLoader(\n    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n)\ndocs = loader.load()\n\n# Sort the list based on the URLs and get the text\nd_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\nd_reversed = list(reversed(d_sorted))\nconcatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n    [doc.page_content for doc in d_reversed]\n)\n\nAPI Reference:\nRecursiveUrlLoader\n\nLLMs\u00b6\nCode solution\u00b6\nFirst, we will try OpenAI and Claude3 with function calling.\nWe will create a code_gen_chain w/ either OpenAI or Claude and test them here.\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\n### OpenAI\n\n# Grader prompt\ncode_gen_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\n# Data model\nclass code(BaseModel):\n    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n\n    prefix: str = Field(description=\"Description of the problem and approach\")\n    imports: str = Field(description=\"Code block import statements\")\n    code: str = Field(description=\"Code block not including import statements\")\n\nexpt_llm = \"gpt-4o-mini\"\nllm = ChatOpenAI(temperature=0, model=expt_llm)\ncode_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\nquestion = \"How do I build a RAG chain in LCEL?\"\nsolution = code_gen_chain_oai.invoke(\n    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n)\nsolution\n\nWeb Research (STORM)\n\nWeb Research (STORM)\u00b6\nSTORM is a research assistant designed by Shao, et. al that extends the idea of \"outline-driven RAG\" for richer article generation.\nSTORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles:\n\nCreating an outline (planning) by querying similar topics helps improve coverage.\nMulti-perspective, grounded (in search) conversation simulation helps increase the reference count and information density. \n\nThe control flow looks like the diagram below.\n\nSTORM has a few main stages:\n\nGenerate initial outline + Survey related subjects\nIdentify distinct perspectives\n\"Interview subject matter experts\" (role-playing LLMs)\nRefine outline (using references)\nWrite sections, then write article\n\nThe expert interviews stage occurs between the role-playing article writer and a research expert. The \"expert\" is able to query external knowledge and respond to pointed questions, saving cited sources to a vectorstore so that the later refinement stages can synthesize the full article.\nThere are a couple hyperparameters you can set to restrict the (potentially) infinite research breadth:\nN: Number of perspectives to survey / use (Steps 2->3)\nM: Max number of conversation turns in step (Step 3)\nSetup\u00b6\nFirst, let's install the required packages and set our API keys\n%%capture --no-stderr\n%pip install -U langchain_community langchain_openai langchain_fireworks langgraph wikipedia duckduckgo-search tavily-python\n\n# Uncomment if you want to draw the pretty graph diagrams.\n# If you are on MacOS, you will need to run brew install graphviz before installing and update some environment flags\n# ! brew install graphviz\n# !CFLAGS=\"-I $(brew --prefix graphviz)/include\" LDFLAGS=\"-L $(brew --prefix graphviz)/lib\" pip install -U pygraphviz\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if os.environ.get(var):\n        return\n    os.environ[var] = getpass.getpass(var + \":\")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nSelect LLMs\u00b6\nWe will have a faster LLM do most of the work, but a slower, long-context model to distill the conversations and write the final report.\nfrom langchain_openai import ChatOpenAI\n\nfast_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n# Uncomment for a Fireworks model\n# fast_llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", max_tokens=32_000)\nlong_context_llm = ChatOpenAI(model=\"gpt-4o\")\n\nAPI Reference:\nChatOpenAI\n\nGenerate Initial Outline\u00b6\nFor many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial\noutline to be refined after our research. Below, we will use our \"fast\" llm to generate the outline.\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\nfrom typing import List, Optional\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\nfrom pydantic import BaseModel, Field\n\ndirect_gen_outline_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n        ),\n        (\"user\", \"{topic}\"),\n    ]\n)\n\nclass Subsection(BaseModel):\n    subsection_title: str = Field(..., title=\"Title of the subsection\")\n    description: str = Field(..., title=\"Content of the subsection\")\n\n    @property\n    def as_str(self) -> str:\n        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n\n6. In technical terms, LangGraph enables the creation of Directed Acyclic Graphs (DAGs) for LLM applications.\n\n7. It's particularly useful for building more complex LLM agents and multi-agent systems.\n\nLangGraph seems to be an advanced tool that builds upon LangChain to provide more sophisticated capabilities for creating stateful and multi-actor LLM applications. It's especially valuable if you're looking to create complex agent systems or applications that require maintaining state across interactions.\n\nIs there any specific aspect of LangGraph you'd like to know more about? I'd be happy to dive deeper into any particular area of interest.\n\nReview this call's LangSmith trace to see the exact work that was done in the above call. Notice that the state is loaded in the first step so that your chatbot can continue where it left off.\nCongrats! You've used an interrupt to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential UIs you can create with your AI systems. Since we have already added a checkpointer, the graph can be paused indefinitely and resumed at any time as if nothing had happened.\nNext, we'll explore how to further customize the bot's behavior using custom state updates.\nBelow is a copy of the code you used in this section. The only difference between this and the previous parts is the addition of the interrupt_before argument.\n\nFull Code\n\nfrom typing import Annotated\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.messages import BaseMessage\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\ntool = TavilySearchResults(max_results=2)\ntools = [tool]\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.set_entry_point(\"chatbot\")\n\nmemory = MemorySaver()\ngraph = graph_builder.compile(\n    checkpointer=memory,\n    # This is new!\n    interrupt_before=[\"tools\"],\n    # Note: can also interrupt __after__ actions, if desired.\n    # interrupt_after=[\"tools\"]\n)\n\nPart 5: Manually Updating the State\u00b6\nIn the previous section, we showed how to interrupt a graph so that a human could inspect its actions. This lets the human read the state, but if they want to change their agent's course, they'll need to have write access.\nThankfully, LangGraph lets you manually update state! Updating the state lets you control the agent's trajectory by modifying its actions (even modifying the past!). This capability is particularly useful when you want to correct the agent's mistakes, explore alternative paths, or guide the agent towards a specific goal.\nWe'll show how to update a checkpointed state below. As before, first, define your graph. We'll reuse the exact same graph as before.\nfrom typing import Annotated\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)", "type": "ToolMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "The documentation provided does not directly offer a simple code example for a React agent. Instead, it delves into complex concepts and implementations involving LangGraph and LangChain, focusing on adaptive RAG (Retrieval-Augmented Generation), code generation with self-correction, and web research with STORM. These concepts are advanced and tailored towards creating sophisticated AI agents that can handle a variety of tasks, including routing queries, generating code based on documentation, and conducting web research to generate comprehensive articles.\n\nGiven the complexity and breadth of the information, if you're looking for a specific example of how to implement a React agent using these technologies, it might be helpful to narrow down the request. For instance, are you interested in creating a React-based web interface that interacts with an AI model for specific tasks like answering questions, generating code, or conducting research? Or are you looking for guidance on how to integrate LangGraph or LangChain functionalities within a React application?\n\nFor a basic example of integrating AI functionalities within a React application, you would typically start by setting up your React app, then use API calls to interact with your AI model or service. Below is a simplified example of how you might structure a React component to send a query to an AI service and display the response:\n\n```jsx\nimport React, { useState } from 'react';\nimport axios from 'axios';\n\nfunction AIQueryComponent() {\n  const [query, setQuery] = useState('');\n  const [response, setResponse] = useState('');\n\n  const handleQueryChange = (event) => {\n    setQuery(event.target.value);\n  };\n\n  const handleSubmit = async (event) => {\n    event.preventDefault();\n    try {\n      const result = await axios.post('YOUR_AI_SERVICE_ENDPOINT', {\n        query: query,\n      });\n      setResponse(result.data.answer); // Adjust according to the response structure\n    } catch (error) {\n      console.error('Error fetching response from AI service:', error);\n    }\n  };\n\n  return (\n    <div>\n      <form onSubmit={handleSubmit}>\n        <input\n          type=\"text\"\n          value={query}\n          onChange={handleQueryChange}\n          placeholder=\"Ask a question\"\n        />\n        <button type=\"submit\">Submit</button>\n      </form>\n      <p>Response: {response}</p>\n    </div>\n  );\n}\n\nexport default AIQueryComponent;\n```\n\nThis example assumes you have an endpoint (`YOUR_AI_SERVICE_ENDPOINT`) that accepts queries and returns responses from an AI model. You would need to replace `'YOUR_AI_SERVICE_ENDPOINT'` with your actual service URL and adjust the request and response handling as necessary for your specific use case.\n\nIf you have a more specific scenario or use case in mind, please provide additional details so I can offer more targeted assistance.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "Creating a LangGraph \"react agent\" involves designing an agent that can dynamically respond or \"react\" to inputs or changes in its environment, leveraging the capabilities of LangGraph for complex decision-making and interaction patterns. While the documentation provided doesn't explicitly detail a \"react agent\" in the context of a simple code example, the principles behind LangGraph can be applied to create such an agent.\n\nA LangGraph react agent would typically involve the following steps:\n\n1. **Define the Agent's Goals and Capabilities**: Determine what the agent is supposed to react to and what actions it can take in response. This could range from answering questions, performing web searches, generating code, or even more complex tasks like conducting research or generating comprehensive articles.\n\n2. **Set Up LangGraph**: Install the necessary packages and set up your environment to use LangGraph. This includes setting API keys for services like OpenAI, Cohere, or any other APIs your agent will use.\n\n3. **Create a Document Index**: If your agent needs to retrieve information from a specific set of documents, you'll need to create an index. This involves loading documents, splitting them into manageable parts, and adding them to a vector store for efficient retrieval.\n\n4. **Design the Interaction Flow**: Use LangGraph to define the flow of interactions. This could involve routing queries to different sources based on their content, using structured prompts to generate responses, or implementing a multi-step process for gathering and synthesizing information.\n\n5. **Implement Reactivity**: Design your agent to be reactive by using LangGraph's capabilities to analyze inputs, decide on the best course of action, and generate appropriate responses. This might involve conditional logic, querying external sources, and dynamically generating content based on the current context.\n\n6. **Test and Refine**: Test your agent with a variety of inputs to ensure it reacts appropriately. Use feedback to refine its decision-making processes and improve its responses.\n\nHere's a conceptual example of how you might structure part of a LangGraph react agent. This example is hypothetical and focuses on the conceptual approach rather than specific code:\n\n```python\nfrom langchain.llms import OpenAI\nfrom langchain.graphs import SimpleGraph\nfrom langchain.nodes import LLMNode, DecisionNode\n\n# Initialize your LLM\nllm = OpenAI(api_key=\"your_openai_api_key\")\n\n# Define a function to decide how to handle a query\ndef route_query(query):\n    if \"recent events\" in query:\n        return \"web_search\"\n    elif \"code example\" in query:\n        return \"generate_code\"\n    else:\n        return \"default_response\"\n\n# Create nodes for your graph\nquery_router = DecisionNode(decision_function=route_query)\nweb_search_node = LLMNode(llm=llm, prompt_template=\"Search the web for: {query}\")\ncode_generation_node = LLMNode(llm=llm, prompt_template=\"Generate code for: {query}\")\ndefault_response_node = LLMNode(llm=llm, prompt_template=\"I'm not sure how to handle: {query}\")\n\n# Set up the graph\ngraph = SimpleGraph(start_node=query_router)\ngraph.add_edge(query_router, web_search_node, condition=lambda x: x == \"web_search\")\ngraph.add_edge(query_router, code_generation_node, condition=lambda x: x == \"generate_code\")\ngraph.add_edge(query_router, default_response_node, condition=lambda x: x == \"default_response\")\n\n# Example query\nresponse = graph.run(\"How do I implement a binary search in Python?\")\nprint(response)\n```\n\nThis example outlines a basic structure for a react agent using decision nodes to route queries and LLM nodes to generate responses. The actual implementation would depend on the specific requirements of your agent, including the APIs and data sources it needs to interact with.", "type": "AIMessage", "additional_kwargs": {}}}
{"error": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}"}
{"node": "agent", "message": {"content": "Displaying tool messages and AI messages on your frontend from an agent involves capturing these messages as they are generated and then sending them to the frontend for display. This process typically requires a backend to handle the agent's operations and a communication channel to send data to the frontend. Here's a general approach to achieve this:\n\n### 1. Capture Messages in the Backend\n\nYour agent, running on the backend, interacts with various tools and AI models, generating tool messages (e.g., logs, errors, status updates) and AI messages (e.g., responses, decisions). You need to capture these messages as they occur. This can be done by wrapping your tool and AI interactions in functions that log or store these messages.\n\nFor example, in Python:\n\n```python\ndef execute_tool(query):\n    # Simulate tool execution\n    tool_message = f\"Executing tool with query: {query}\"\n    # Capture the tool message\n    print(tool_message)  # For demonstration; in practice, you'd store this message\n    # Simulate tool response\n    return \"Tool response\"\n\ndef ask_ai(question):\n    # Simulate AI interaction\n    ai_message = f\"AI responding to: {question}\"\n    # Capture the AI message\n    print(ai_message)  # For demonstration; in practice, you'd store this message\n    # Simulate AI response\n    return \"AI response\"\n```\n\n### 2. Store or Log Messages\n\nInstead of just printing messages, you should store them in a way that they can be easily retrieved and sent to the frontend. This could involve appending messages to a database, an in-memory data structure, or a log file, depending on your application's requirements and scale.\n\n### 3. Send Messages to the Frontend\n\nThere are several ways to send data from the backend to the frontend, including:\n\n- **Polling**: The frontend periodically sends requests to the backend to check for new messages.\n- **Long Polling**: The frontend sends a request to the backend, which holds the request open until new messages are available.\n- **WebSockets**: Establish a persistent, two-way connection between the frontend and backend. This allows the backend to push messages to the frontend in real-time.\n- **Server-Sent Events (SSE)**: A standard describing how servers can initiate data transmission towards browser clients once an initial client connection has been established.\n\n### 4. Display Messages on the Frontend\n\nOnce the frontend receives these messages, you can display them using your frontend framework or library of choice (e.g., React, Angular, Vue.js). For a simple example using React:\n\n```jsx\nfunction MessagesDisplay({ messages }) {\n  return (\n    <div>\n      {messages.map((msg, index) => (\n        <div key={index}>{msg}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Example with WebSockets\n\nHere's a very simplified example using WebSockets:\n\n**Backend (Python with Flask-SocketIO):**\n\n```python\nfrom flask import Flask\nfrom flask_socketio import SocketIO, emit\n\napp = Flask(__name__)\nsocketio = SocketIO(app)\n\n@socketio.on('request_message')\ndef handle_message(data):\n    # Process the request, capture messages\n    tool_message = execute_tool(data['query'])\n    ai_message = ask_ai(data['question'])\n    # Emit messages back to the client\n    emit('update', {'tool_message': tool_message, 'ai_message': ai_message})\n\nif __name__ == '__main__':\n    socketio.run(app)\n```\n\n**Frontend (React with Socket.IO Client):**\n\n```jsx\nimport React, { useEffect, useState } from 'react';\nimport io from 'socket.io-client';\n\nconst socket = io('http://localhost:5000');\n\nfunction App() {\n  const [messages, setMessages] = useState([]);\n\n  useEffect(() => {\n    socket.on('update', data => {\n      setMessages(messages => [...messages, data.tool_message, data.ai_message]);\n    });\n  }, []);\n\n  return (\n    <div>\n      <MessagesDisplay messages={messages} />\n    </div>\n  );\n}\n```\n\nThis example demonstrates a basic setup. Your actual implementation will likely need to handle more complex scenarios, including error handling, user authentication, and optimizing for performance and scalability.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "Hello! How can I assist you today?", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "To correctly display tool messages and AI messages on your frontend from an agent, you'll need to implement a system that captures these messages from your backend (where the agent operates) and then sends them to the frontend for display. Here's a step-by-step guide on how to achieve this:\n\n### Step 1: Capture Messages in the Backend\n\nWhen your agent interacts with tools or generates AI messages, you need to capture these interactions. This can be done by wrapping your tool interactions and AI message generation in functions that log or store these messages.\n\nFor example, in a Node.js backend, you might have functions like these:\n\n```javascript\nfunction executeTool(query) {\n  const toolMessage = `Executing tool with query: ${query}`;\n  console.log(toolMessage); // For demonstration; ideally, you'd store this in a log or database\n  // Simulate tool execution and response\n  return \"Tool response\";\n}\n\nfunction generateAIMessage(question) {\n  const aiMessage = `AI responding to: ${question}`;\n  console.log(aiMessage); // For demonstration; ideally, you'd store this in a log or database\n  // Simulate AI response\n  return \"AI response\";\n}\n```\n\n### Step 2: Store or Log Messages\n\nInstead of just logging messages to the console, you should store them in a manner that they can be easily retrieved and sent to the frontend. This could involve using a database, an in-memory store, or a logging system, depending on your application's needs.\n\n### Step 3: Send Messages to the Frontend\n\nTo send messages from the backend to the frontend, you can use several approaches, such as:\n\n- **WebSockets**: This is a popular choice for real-time applications. WebSockets allow for a two-way interactive communication session between the user's browser and a server. With WebSockets, you can push messages from the server to the frontend as soon as they are generated.\n- **Server-Sent Events (SSE)**: This is a simple unidirectional protocol that allows the server to send events to the client. It's a good choice for applications that only need to send data from the server to the client.\n- **Polling or Long Polling**: In polling, the frontend periodically sends requests to the backend to check for new messages. Long polling is a variation where the server holds the request open until new data is available.\n\n### Step 4: Display Messages on the Frontend\n\nOnce you have the infrastructure to send messages to the frontend, you can display them using your frontend framework or library of choice (e.g., React, Angular, Vue.js). Here's a simple example using React and WebSockets:\n\n```jsx\nimport React, { useEffect, useState } from 'react';\nimport { io } from 'socket.io-client';\n\nconst socket = io('YOUR_BACKEND_URL');\n\nfunction App() {\n  const [messages, setMessages] = useState([]);\n\n  useEffect(() => {\n    socket.on('toolMessage', message => {\n      setMessages(prevMessages => [...prevMessages, message]);\n    });\n\n    socket.on('aiMessage', message => {\n      setMessages(prevMessages => [...prevMessages, message]);\n    });\n  }, []);\n\n  return (\n    <div>\n      {messages.map((msg, index) => (\n        <p key={index}>{msg}</p>\n      ))}\n    </div>\n  );\n}\n\nexport default App;\n```\n\nIn this example, the frontend listens for `toolMessage` and `aiMessage` events from the backend and updates the state accordingly, which triggers a re-render to display the new messages.\n\nThis approach ensures that your frontend dynamically displays messages from your agent as they are generated, providing real-time feedback to the user.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "", "type": "AIMessage", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_77y3syklW02Ucvd4Bce6QPJ2", "function": {"arguments": "{\"query\":\"LangGraph React Agent tool messages\"}", "name": "retrieve_docs"}, "type": "function"}]}}}
{"node": "action", "message": {"content": "21\nTroubleshooting FastAPI Deployments\nResolving issues with deploying a React app with a FastAPI backend.\n\n22\nAnalyzing Data with LangChain Agents\nUsing LangChain agents to interact with Pandas and Spark DataFrames for data exploration.\n\n23\nImplementing the OpenAI Chat API\nImplementing the OpenAI chat completion API and understanding the required inputs and outputs.\n\n24\nComparing LangChain and LLMIndex\nEvaluating the differences between LangChain and LLMIndex, including their UI support for Markdown.\n\n25\nSuppressing Tools in AgentExecutor\nTemporarily disabling tools in an AgentExecutor for a fixed number of invocations.\n\nPhase 2: Labeling\u00b6\nNow that we have our taxonomy, it's time to label a subset of our data to train a classifier.\nInput classification can be useful for anything from in-line prompt optimization (tailor the prompt for each classified intent), to system improvements (identifying categories for which the system doesn't produce good responses) to product analytics (understand which intent categories could be improved to drive profits).\nThe problem is that LLM-based tagging can be expensive.\nEmbeddings can be ~100x cheaper to compute, and a simple logistic regression classifier on top of that would add negligible cost.\nLet's tag and train a classifier!\nLabel Training Data\u00b6\nUse an LLM to label the data in a fully-automated fashion. For better accuracy, you can sample a portion of the results to label by hand as well to verify the quality.\nlabeling_prompt = hub.pull(\"wfh/tnt-llm-classify\")\n\nlabeling_llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", max_tokens_to_sample=2000)\nlabeling_llm_chain = (labeling_prompt | labeling_llm | StrOutputParser()).with_config(\n    run_name=\"ClassifyDocs\"\n)\n\ndef parse_labels(output_text: str) -> Dict:\n    \"\"\"Parse the generated labels from the predictions.\"\"\"\n    category_matches = re.findall(\n        r\"\\s*<category>(.*?)</category>.*\",\n        output_text,\n        re.DOTALL,\n    )\n    categories = [{\"category\": category.strip()} for category in category_matches]\n    if len(categories) > 1:\n        logger.warning(f\"Multiple selected categories: {categories}\")\n    label = categories[0]\n    stripped = re.sub(r\"^\\d+\\.\\s*\", \"\", label[\"category\"]).strip()\n    return {\"category\": stripped}\n\nlabeling_chain = labeling_llm_chain | parse_labels\n\nfinal_taxonomy = step[\"__end__\"][\"clusters\"][-1]\nxml_taxonomy = format_taxonomy(final_taxonomy)\nresults = labeling_chain.batch(\n    [\n        {\n            \"content\": doc[\"content\"],\n            \"taxonomy\": xml_taxonomy,\n        }\n        for doc in docs\n    ],\n    {\"max_concurrency\": 5},\n    return_exceptions=True,\n)\n# Update the docs to include the categories\nupdated_docs = [{**doc, **category} for doc, category in zip(docs, results)]\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OPENAI_API_KEY: \")\n\nfrom langchain_openai import OpenAIEmbeddings\n\n# Consider using other embedding models here too!\nencoder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\nvectors = encoder.embed_documents([doc[\"content\"] for doc in docs])\nembedded_docs = [{**doc, \"embedding\": v} for doc, v in zip(updated_docs, vectors)]\n\nAPI Reference:\nOpenAIEmbeddings\n\nTrain Classifier\u00b6\nNow that we've extracted the features from the text, we can generate the classifier on them.\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\n# Create a dictionary mapping category names to their indices in the taxonomy\ncategory_to_index = {d[\"name\"]: i for i, d in enumerate(final_taxonomy)}\ncategory_to_index[\"Other\"] = len(category_to_index)\n# Convert category strings to numeric labels\nlabels = [\n    category_to_index.get(d[\"category\"], category_to_index[\"Other\"])\n    for d in embedded_docs\n]\n\nlabel_vectors = [d[\"embedding\"] for d in embedded_docs]\n\nHow to create a ReAct agent\nHow to add memory to a ReAct agent\nHow to add a custom system prompt to a ReAct agent\nHow to add human-in-the-loop processes to a ReAct agent\nHow to create prebuilt ReAct agent from scratch\n\nTroubleshooting\u00b6\nThe Error Reference page contains guides around resolving common errors you may find while building with LangChain. Errors referenced below will have an lc_error_code property corresponding to one of the below codes when they are thrown in code.\n\nGRAPH_RECURSION_LIMIT\nINVALID_CONCURRENT_GRAPH_UPDATE\nINVALID_GRAPH_NODE_RETURN_VALUE\nMULTIPLE_SUBGRAPHS\n\nComments\n\nTutorials\n\nTutorials\u00b6\nWelcome to the LangGraph Tutorials! These notebooks introduce LangGraph through building various language agents and applications.\nQuick Start\u00b6\nLearn the basics of LangGraph through a comprehensive quick start in which you will build an agent from scratch.\n\nQuick Start\n\nUse cases\u00b6\nLearn from example implementations of graphs designed for specific scenarios and that implement common design patterns.\nChatbots\u00b6\n\nCustomer Support: Build a customer support chatbot to manage flights, hotel reservations, car rentals, and other tasks\nPrompt Generation from User Requirements: Build an information gathering chatbot\nCode Assistant: Build a code analysis and generation assistant\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nLanguage Agent Tree Search: Use reflection and rewards to drive a tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments\n\n6. In technical terms, LangGraph enables the creation of Directed Acyclic Graphs (DAGs) for LLM applications.\n\n7. It's particularly useful for building more complex LLM agents and multi-agent systems.\n\nLangGraph seems to be an advanced tool that builds upon LangChain to provide more sophisticated capabilities for creating stateful and multi-actor LLM applications. It's especially valuable if you're looking to create complex agent systems or applications that require maintaining state across interactions.\n\nIs there any specific aspect of LangGraph you'd like to know more about? I'd be happy to dive deeper into any particular area of interest.\n\nReview this call's LangSmith trace to see the exact work that was done in the above call. Notice that the state is loaded in the first step so that your chatbot can continue where it left off.\nCongrats! You've used an interrupt to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential UIs you can create with your AI systems. Since we have already added a checkpointer, the graph can be paused indefinitely and resumed at any time as if nothing had happened.\nNext, we'll explore how to further customize the bot's behavior using custom state updates.\nBelow is a copy of the code you used in this section. The only difference between this and the previous parts is the addition of the interrupt_before argument.\n\nFull Code\n\nfrom typing import Annotated\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.messages import BaseMessage\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\ntool = TavilySearchResults(max_results=2)\ntools = [tool]\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.set_entry_point(\"chatbot\")\n\nmemory = MemorySaver()\ngraph = graph_builder.compile(\n    checkpointer=memory,\n    # This is new!\n    interrupt_before=[\"tools\"],\n    # Note: can also interrupt __after__ actions, if desired.\n    # interrupt_after=[\"tools\"]\n)\n\nPart 5: Manually Updating the State\u00b6\nIn the previous section, we showed how to interrupt a graph so that a human could inspect its actions. This lets the human read the state, but if they want to change their agent's course, they'll need to have write access.\nThankfully, LangGraph lets you manually update state! Updating the state lets you control the agent's trajectory by modifying its actions (even modifying the past!). This capability is particularly useful when you want to correct the agent's mistakes, explore alternative paths, or guide the agent towards a specific goal.\nWe'll show how to update a checkpointed state below. As before, first, define your graph. We'll reuse the exact same graph as before.\nfrom typing import Annotated\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\nWeb Research (STORM)\n\nWeb Research (STORM)\u00b6\nSTORM is a research assistant designed by Shao, et. al that extends the idea of \"outline-driven RAG\" for richer article generation.\nSTORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles:\n\nCreating an outline (planning) by querying similar topics helps improve coverage.\nMulti-perspective, grounded (in search) conversation simulation helps increase the reference count and information density. \n\nThe control flow looks like the diagram below.\n\nSTORM has a few main stages:\n\nGenerate initial outline + Survey related subjects\nIdentify distinct perspectives\n\"Interview subject matter experts\" (role-playing LLMs)\nRefine outline (using references)\nWrite sections, then write article\n\nThe expert interviews stage occurs between the role-playing article writer and a research expert. The \"expert\" is able to query external knowledge and respond to pointed questions, saving cited sources to a vectorstore so that the later refinement stages can synthesize the full article.\nThere are a couple hyperparameters you can set to restrict the (potentially) infinite research breadth:\nN: Number of perspectives to survey / use (Steps 2->3)\nM: Max number of conversation turns in step (Step 3)\nSetup\u00b6\nFirst, let's install the required packages and set our API keys\n%%capture --no-stderr\n%pip install -U langchain_community langchain_openai langchain_fireworks langgraph wikipedia duckduckgo-search tavily-python\n\n# Uncomment if you want to draw the pretty graph diagrams.\n# If you are on MacOS, you will need to run brew install graphviz before installing and update some environment flags\n# ! brew install graphviz\n# !CFLAGS=\"-I $(brew --prefix graphviz)/include\" LDFLAGS=\"-L $(brew --prefix graphviz)/lib\" pip install -U pygraphviz\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if os.environ.get(var):\n        return\n    os.environ[var] = getpass.getpass(var + \":\")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nSelect LLMs\u00b6\nWe will have a faster LLM do most of the work, but a slower, long-context model to distill the conversations and write the final report.\nfrom langchain_openai import ChatOpenAI\n\nfast_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n# Uncomment for a Fireworks model\n# fast_llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", max_tokens=32_000)\nlong_context_llm = ChatOpenAI(model=\"gpt-4o\")\n\nAPI Reference:\nChatOpenAI\n\nGenerate Initial Outline\u00b6\nFor many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial\noutline to be refined after our research. Below, we will use our \"fast\" llm to generate the outline.\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\nfrom typing import List, Optional\n\nfrom langchain_core.prompts import ChatPromptTemplate\n\nfrom pydantic import BaseModel, Field\n\ndirect_gen_outline_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n        ),\n        (\"user\", \"{topic}\"),\n    ]\n)\n\nclass Subsection(BaseModel):\n    subsection_title: str = Field(..., title=\"Title of the subsection\")\n    description: str = Field(..., title=\"Content of the subsection\")\n\n    @property\n    def as_str(self) -> str:\n        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()", "type": "ToolMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "The documentation does not directly address how LangGraph React (ReAct) agent tool messages work. However, based on the information available and the context of LangGraph, we can infer some general principles about how tool messages might be handled within a LangGraph-based agent, especially in a React (or ReAct) environment.\n\nLangGraph is designed for building complex language agents and applications, utilizing Directed Acyclic Graphs (DAGs) for structuring interactions and processes. This structure allows for the creation of stateful and multi-actor LLM (Large Language Model) applications, which can include various tools and components interacting within the agent's workflow.\n\n### Handling Tool Messages in a LangGraph ReAct Agent\n\n1. **Integration with LangGraph**: When integrating tools within a LangGraph-based agent, each tool can be represented as a node within the graph. These nodes can perform specific tasks, such as fetching data, processing information, or interacting with external APIs.\n\n2. **Tool Message Generation**: As tools execute their tasks, they can generate messages. These messages might include status updates, results from data processing, or any errors encountered during execution. In a ReAct environment, these messages need to be captured and managed effectively to ensure they can be displayed or utilized within the frontend.\n\n3. **State Management**: LangGraph allows for the management of state across interactions. This state management capability can be leveraged to store and track tool messages as part of the agent's overall state. By maintaining a record of these messages, the agent can provide contextually relevant information to the user or use the information to make decisions in subsequent steps.\n\n4. **Frontend Integration**: To display tool messages on a React frontend, the agent's backend (where LangGraph operates) would need to communicate these messages to the frontend. This could be achieved through WebSockets, Server-Sent Events (SSE), or other real-time communication protocols, allowing the frontend to receive and display messages as they are generated.\n\n5. **User Interface Considerations**: On the frontend, React components can be designed to listen for incoming tool messages and update the UI accordingly. This might involve displaying notifications, updating data visualizations, or triggering new UI flows based on the content of the messages.\n\n6. **Customization and Configuration**: LangGraph's flexibility allows for the customization of how tool messages are generated, processed, and handled. This means that developers can configure their agents to handle tool messages in a way that best fits their application's requirements, whether that involves filtering messages, aggregating information, or triggering specific actions based on message content.\n\nWhile the documentation does not provide specific details on ReAct agent tool messages, understanding LangGraph's capabilities and structure offers a foundation for implementing and managing tool messages within a LangGraph-based agent, especially in applications utilizing React for the frontend.", "type": "AIMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "Hello! How can I assist you today?", "type": "AIMessage", "additional_kwargs": {}}}
{"error": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}"}
{"node": "agent", "message": {"content": "", "type": "AIMessage", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_Kq3VIA8PBq3DxFywJaSjGNgn", "function": {"arguments": "{\"query\":\"how to view all message types in logs from LangGraph agents including tools\"}", "name": "retrieve_docs"}, "type": "function"}]}}}
{"node": "action", "message": {"content": "And it works!\u00b6\nRetries are an easy way to reduce function calling failures. While retrying may become unnecessary with more powerful LLMs, data validation is important to control how LLMs interact with the rest of your software stack.\nIf you notice high retry rates (using an observability tool like LangSmith), you can set up a rule to send the failure cases to a dataset alongside the corrected values and then automatically program those into your prompts or schemas (or use them as few-shots to have semantically relevant demonstrations).\nComments\n\nHow-to Guides\n\nHow-to Guides\u00b6\nWelcome to the LangGraph how-to guides! These guides provide practical, step-by-step instructions for accomplishing key tasks in LangGraph.\nControllability\u00b6\nLangGraph offers a high level of control over the execution of your graph.\nThese how-to guides show how to achieve that controllability.\n\nHow to create branches for parallel execution\nHow to create map-reduce branches for parallel execution\nHow to control graph recursion limit\n\nPersistence\u00b6\nLangGraph Persistence makes it easy to persist state across graph runs (thread-level persistence) and across threads (cross-thread persistence). These how-to guides show how to add persistence to your graph.\n\nHow to add thread-level persistence to your graph\nHow to add thread-level persistence to subgraphs\nHow to add cross-thread persistence to your graph\nHow to use Postgres checkpointer for persistence\nHow to create a custom checkpointer using MongoDB\nHow to create a custom checkpointer using Redis\n\nMemory\u00b6\nLangGraph makes it easy to manage conversation memory in your graph. These how-to guides show how to implement different strategies for that.\n\nHow to manage conversation history\nHow to delete messages\nHow to add summary conversation memory\n\nHuman in the Loop\u00b6\nHuman-in-the-loop functionality allows\nyou to involve humans in the decision-making process of your graph. These how-to guides show how to implement human-in-the-loop workflows in your graph.\n\nHow to add breakpoints\nHow to add dynamic breakpoints\nHow to edit graph state\nHow to wait for user input\nHow to view and update past graph state\nReview tool calls\n\nStreaming\u00b6\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\nHow to stream full state of your graph\nHow to stream state updates of your graph\nHow to stream LLM tokens\nHow to stream LLM tokens without LangChain models\nHow to stream custom data\nHow to configure multiple streaming modes at the same time\nHow to stream events from within a tool\nHow to stream events from within a tool without LangChain models\nHow to stream events from the final node\nHow to stream from subgraphs\nHow to disable streaming for models that don't support it\n\nTool calling\u00b6\nTool calling is a type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message. \nThese how-to guides show common patterns for tool calling with LangGraph:\n\nHow to call tools using ToolNode\nHow to handle tool calling errors\nHow to pass runtime values to tools\nHow to pass config to tools\nHow to handle large numbers of tools\n\nSubgraphs\u00b6\nSubgraphs allow you to reuse an existing graph from another graph. These how-to guides show how to use subgraphs:\n\nHow to add and use subgraphs\nHow to view and update state in subgraphs\nHow to transform inputs and outputs of a subgraph\n\nState Management\u00b6\n\nUse Pydantic model as state\nHave a separate input and output schema\nPass private state between nodes inside the graph\n\nOther\u00b6\n\nHow to run graph asynchronously\nHow to visualize your graph\nHow to add runtime configuration to your graph\nHow to use a Pydantic model as your state\nHow to add node retries\nHow to force function calling agent to structure output\nHow to pass custom LangSmith run ID for graph runs\nHow to return state before hitting recursion limit\n\nPrebuilt ReAct Agent\u00b6\nThe LangGraph prebuilt ReAct agent is pre-built implementation of a tool calling agent.\nOne of the big benefits of LangGraph is that you can easily create your own agent architectures. So while it's fine to start here to build an agent quickly, we would strongly recommend learning how to build your own agent so that you can take full advantage of LangGraph.\nThese guides show how to use the prebuilt ReAct agent:\n\nDo you have a specific type of autonomous agent in mind, or would you like some suggestions for beginner-friendly autonomous agent projects to start with?\n\nNotice that the graph resumed execution from the **action** node. You can tell this is the case since the first value printed above is the response from our search engine tool.\nCongratulations! You've now used time-travel checkpoint traversal in LangGraph. Being able to rewind and explore alternative paths opens up a world of possibilities for debugging, experimentation, and interactive applications.\nConclusion\u00b6\nCongrats! You've completed the intro tutorial and built a chat bot in LangGraph that supports tool calling, persistent memory, human-in-the-loop interactivity, and even time-travel!\nThe LangGraph documentation is a great resource for diving deeper into the library's capabilities.\nComments\n\nWe can see the results benchmarked against GPT-4o and Llama-3-70b using Custom agent (as shown here) and ReAct.\n\nThe local custom agent performs well in terms of tool calling reliability: it follows the expected reasoning traces.\nHowever, the answer accuracy performance lags the larger models with custom agent implementations.\nComments\n\nHome\n\n\ud83e\udd9c\ud83d\udd78\ufe0fLangGraph\u00b6\n\n\u26a1 Building language agents as graphs \u26a1\n\nNote\nLooking for the JS version? Click here (JS docs).\n\nOverview\u00b6\nLangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\nTo learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available for free here.\nKey Features\u00b6\n\nCycles and Branching: Implement loops and conditionals in your apps.\nPersistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\nHuman-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.\nStreaming Support: Stream outputs as they are produced by each node (including token streaming).\nIntegration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).\n\nInstallation\u00b6\npip install -U langgraph\n\nExample\u00b6\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\nLet's take a look at a simple example of an agent that can use a search tool.\npip install langchain-anthropic\n\nexport ANTHROPIC_API_KEY=sk-...\n\nOptionally, we can set up LangSmith for best-in-class observability.\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=lsv2_sk_...\n\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)", "type": "ToolMessage", "additional_kwargs": {}}}
{"node": "agent", "message": {"content": "The documentation does not directly address how to view all message types in logs from LangGraph agents, including tools. However, it provides extensive information on various aspects of using LangGraph, such as tool calling, state management, streaming, and human-in-the-loop functionalities. \n\nFor logging and observing message types, especially regarding tool calls and agent interactions, you might consider integrating observability tools like LangSmith. LangSmith is mentioned in the context of setting up for best-in-class observability, which suggests it could be used to monitor and log different types of messages and interactions within LangGraph agents. \n\nTo specifically view all message types in logs, including those from tools, you would likely need to implement custom logging within your LangGraph agents or utilize LangSmith's observability features. This could involve capturing and logging the output of tool calls, agent decisions, and other interactions as part of your agent's execution flow.\n\nIf you're looking for detailed logging of all interactions, including tool calls, consider the following approaches based on the provided documentation:\n\n1. **Implement Custom Logging**: Extend your LangGraph agents to include logging at key points of interaction, such as before and after tool calls, and when processing input and output messages. This would give you detailed logs of all message types and interactions.\n\n2. **Use LangSmith for Observability**: As mentioned, setting up LangSmith for observability could provide a comprehensive view of your agent's operations, including tool calls and message types. Ensure you have the necessary configurations (`LANGSMITH_TRACING` and `LANGSMITH_API_KEY`) set up for this purpose.\n\n3. **Review Tool Calls**: The documentation mentions the ability to review tool calls within human-in-the-loop workflows. This suggests that there are mechanisms within LangGraph to inspect and potentially log these interactions, which could be adapted for broader logging purposes.\n\n4. **Streaming Support**: For real-time applications, consider leveraging LangGraph's streaming support to log messages as they are produced. This could include streaming outputs from tool calls and other agent interactions.\n\nFor more specific guidance on implementing these approaches, you might need to explore LangGraph's and LangSmith's documentation further or reach out to their support communities.", "type": "AIMessage", "additional_kwargs": {}}}
